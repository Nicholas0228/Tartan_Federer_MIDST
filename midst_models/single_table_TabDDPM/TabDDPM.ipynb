{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TABDDPM: Modelling Tabular Data with Diffusion Models\n",
    "\n",
    "Directly applying diffusion models to general tabular problems can be challenging because data points are typically represented by vectors of heterogeneous features. The inherent heterogeneity of tabular data complicates accurate modeling, as individual features can vary widely in nature; some may be continuous, while others are discrete. In this notebook, we explore **TabDDPM** — a diffusion model that can be universally applied to tabular datasets and effectively handles both categorical and numerical features.\n",
    "\n",
    "Our primary focus in this work is synthetic data generation, which is in high demand for many tabular tasks. Firstly, tabular datasets are often limited in size, unlike vision or NLP problems where large amounts of additional data are readily available online. Secondly, properly generated synthetic datasets do not contain actual user data, thus avoiding GDPR-like regulations and allowing for public sharing without compromising anonymity.\n",
    "\n",
    "In this notebook, we work with the ClavaDDPM implementation, which is originally designed for multi-table data synthesis. However, by applying a specific single-table configuration, we can effectively leverage it for single-table synthesis as well. This configuration activates TabDDPM, a component within ClavaDDPM tailored for single-table scenarios.\n",
    "\n",
    "In the following sections, we will delve deeper into the implementation of this method. The notebook is organized as follows:\n",
    "\n",
    "1. [Imports and Setup]()\n",
    "\n",
    "\n",
    "2. [Load Configuration]()\n",
    "\n",
    "\n",
    "3. [Data Loading and Preprocessing]()\n",
    "    \n",
    "    \n",
    "4. [TabDDPM Algorithm]()\n",
    "\n",
    "    4.1. [Overview]()\n",
    "    \n",
    "    4.2. [Model Training]()\n",
    "    \n",
    "    4.3. [Model Sampling]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setup\n",
    "\n",
    "In this section, we import all necessary libraries and modules for setting up the environment. This includes libraries for logging, argument parsing, file path management, and configuration loading. We also import essential packages for data loading, model creation, and training, such as PyTorch and numpy, along with custom modules specific to the ClavaDDPM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import load_multi_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Configuration\n",
    "\n",
    "In this section, we establish the setup for model training by loading the configuration file, which includes the necessary parameters and settings for the training process. The configuration file, stored in `json` format, is read and parsed into a dictionary. We print out the entire configuration file in the code cell below and will explain the hyperparameters in more detail further down to clarify.\n",
    "\n",
    "A sample configuration file is available at `configs/trans.json`, where general parameters can be modified as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"general\": {\n",
      "        \"data_dir\": \"/work4/xiaoyuwu/MIDSTModelsMIA/starter_kits/tabddpm_white_box/train/tabddpm_1\",\n",
      "        \"exp_name\": \"train_1\",\n",
      "        \"workspace_dir\": \"/work4/xiaoyuwu/MIDSTModelsMIA/starter_kits/tabddpm_white_box/train/tabddpm_1/workspace\",\n",
      "        \"sample_prefix\": \"\",\n",
      "        \"test_data_dir\": \"/work4/xiaoyuwu/MIDSTModelsMIA/starter_kits/tabddpm_white_box/train/tabddpm_1\"\n",
      "    },\n",
      "    \"clustering\": {\n",
      "        \"parent_scale\": 1.0,\n",
      "        \"num_clusters\": 50,\n",
      "        \"clustering_method\": \"both\"\n",
      "    },\n",
      "    \"diffusion\": {\n",
      "        \"d_layers\": [\n",
      "            512,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            1024,\n",
      "            512\n",
      "        ],\n",
      "        \"dropout\": 0.0,\n",
      "        \"num_timesteps\": 2000,\n",
      "        \"model_type\": \"mlp\",\n",
      "        \"iterations\": 200000,\n",
      "        \"batch_size\": 4096,\n",
      "        \"lr\": 0.0006,\n",
      "        \"gaussian_loss_type\": \"mse\",\n",
      "        \"weight_decay\": 1e-05,\n",
      "        \"scheduler\": \"cosine\"\n",
      "    },\n",
      "    \"classifier\": {\n",
      "        \"d_layers\": [\n",
      "            128,\n",
      "            256,\n",
      "            512,\n",
      "            1024,\n",
      "            512,\n",
      "            256,\n",
      "            128\n",
      "        ],\n",
      "        \"lr\": 0.0001,\n",
      "        \"dim_t\": 128,\n",
      "        \"batch_size\": 4096,\n",
      "        \"iterations\": 20000\n",
      "    },\n",
      "    \"sampling\": {\n",
      "        \"batch_size\": 20000,\n",
      "        \"classifier_scale\": 1.0\n",
      "    },\n",
      "    \"matching\": {\n",
      "        \"num_matching_clusters\": 1,\n",
      "        \"matching_batch_size\": 1000,\n",
      "        \"unique_matching\": true,\n",
      "        \"no_matching\": false\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load config\n",
    "config_path = \"configs/trans_demo.json\"\n",
    "configs, save_dir = load_configs(config_path)\n",
    "\n",
    "# Display config\n",
    "json_str = json.dumps(configs, indent=4)\n",
    "print(json_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading and Preprocessing\n",
    "\n",
    "In this notebook, we use the Transactions table from the Berka dataset. You can access the Berka dataset files for TabDDPM [here](https://drive.google.com/drive/folders/18KHv3VQuRphMHqZQsQc-x2ALoIiAggA0?usp=drive_link).\n",
    "\n",
    "The BERKA dataset is a comprehensive banking dataset originally released by the Czech bank ČSOB for the Financial Modeling and Analysis (FMA) competition in 1999. It provides detailed financial data on transactions, accounts, loans, credit cards, and demographic information for thousands of customers over multiple years.\n",
    "In this section, we load and preprocess the dataset based on the configuration settings. \n",
    "The following files are needed to be present in the data directory:\n",
    "- `train.csv`: The transactions susbet from the Berka dataset used for training. Note that the id columns (columns ending in \"_id\") should be removed from the training data.\n",
    "- `test.csv`: The transactions susbet from the Berka dataset used for evaluation. Note that the id columns (columns ending in \"_id\") should be removed from the test data.\n",
    "- `trans_label_encoders.pkl`: The label encoders used to encode the transactions table if you are using the already preprocessed data from shared files.\n",
    "- `trans_domain.json`: This file contains the domain information for each column in the transactions table. A sample domain file is available at `configs/trans_domain.json`\n",
    "- `dataset_meta.json`: The configuration file defines the relationships between different tables in the dataset. For single-table synthesis, it should be configured to include only one table. A sample configuration file is available at `configs/dataset_meta.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table name: trans, Total dataframe shape: (20000, 8), Numerical data shape: (20000, 4), Categorical data shape: (20000, 4)\n",
      "\n",
      "==================== We show the keys of the tables dictionary below ====================\n",
      "['trans']\n"
     ]
    }
   ],
   "source": [
    "# Load  dataset\n",
    "# In this step, we load the dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "tables, relation_order, dataset_meta = load_multi_table(configs[\"general\"][\"data_dir\"])\n",
    "print(\"\")\n",
    "\n",
    "# Tables is a dictionary of the multi-table dataset\n",
    "print(\n",
    "    \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "print(list(tables.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TabDDPM Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will describe the design of TabDDPM as well as its main hyperparameters loaded through config, which affect the model’s effectiveness. \n",
    "\n",
    "**TabDDPM:** uses the multinomial diffusion to model the categorical and binary features, and the Gaussian diffusion to model the numerical ones. The model is trained using the diffusion process, which is a continuous-time Markov chain that models the data distribution. In more detail, for a tabular data sample that consists of N numerical featuresand C categorical features with Ki categories each, TabDDPM takes one-hot encoded versions of categorical features as an input, and normalized numerical features. The figure below illustrates the diffusion process for classification problems; t, y and l denote a diffusion timestep, a class label, and logits, respectively.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/1b772284-de6a-44ad-8346-39b5f040cd31\" width=\"1000\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Diffusion models:**  are likelihood-based generative models that handle the data through forward and reverse Markov processes. The forward process gradually adds noise to an initial sample x0 from the data distribution q(x0) sampling noise from the predefined distributions q(xt|xt−1) with variances {β1, ..., βT}.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/6f610e06-ab5b-4974-97ce-9767baf254ea\" width=\"300\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse diffusion proces gradually denoises a latent variable xT∼q(xT) and allows generating new data samples from q(x0). Distributions p(xt−1|xt) are usually unknown and approximated by a neural network with parameters θ.\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/2c641eda-1678-4009-8d6e-88bf2ab24600\" width=\"280\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Gaussian diffusion models:** operate in continuous spaces where forward and reverse processes are characterized by Gaussian distributions:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/c0cfa4a8-9281-4a7a-aaaa-b220ffd05734\" width=\"330\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While in general θ parameters are learned from the data by optimizing a variational lower bound, in practice for Gaussian modeling, this objective can be simplified to the sum of mean-squared errors between εθ(xt ,t) and ε over all timesteps t as follows:\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/61f34373-3890-4785-98c6-6e103bd81950\" width=\"330\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multinomial diffusion models:** are designed to generate categorical data where samples are a one-hot encoded categorical variable with K values. The multinomial forward diffusion process defines q(xt|xt−1) as a categorical distribution that corrupts the data by uniform noise over K classes: \n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://github.com/user-attachments/assets/ced8bc14-9296-4a09-9881-64f90bed537d\" width=\"440\"/>\n",
    "</p>\n",
    "\n",
    "The reverse distribution pθ(xt−1|xt) is parameterized as q(xt−1|xt,xˆ0(xt,t)), where xˆ0 is predicted by a neural network. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "Note that ClavaDDPM introduces relation-aware clustering to model parent-child constraints and leverages diffusion models for controlled tabular data synthesis. However in the single-table synthesis scenario, although we perform the clustering, it won't have an impact how the model is trained or sampled.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the clustering parameters below ====================\n",
      "parent_scale: 1.0\n",
      "num_clusters: 50\n",
      "clustering_method: both\n",
      "\n",
      "Clustering checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "# Display important clustering parameters\n",
    "params_clustering = configs[\"clustering\"]\n",
    "print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "for key, val in params_clustering.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")\n",
    "\n",
    "# Clustering on the multi-table dataset\n",
    "tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "    tables, relation_order, save_dir, configs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important parameters for the training process include:\n",
    "\n",
    "- `d_layers`: the dimension of layers in the diffusion model. \n",
    "- `num_timesteps`: the number of diffusion steps for adding noise and denoising. \n",
    "- `iterations`: the number of training iterations. The default is 10000. Recommended range for tuning: 5000 to 20000.\n",
    "- `batch_size`: the batch size for training. The default is 4096. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the important sampling parameters below ====================\n",
      "d_layers: [512, 1024, 1024, 1024, 1024, 512]\n",
      "dropout: 0.0\n",
      "num_timesteps: 2000\n",
      "model_type: mlp\n",
      "iterations: 200000\n",
      "batch_size: 4096\n",
      "lr: 0.0006\n",
      "gaussian_loss_type: mse\n",
      "weight_decay: 1e-05\n",
      "scheduler: cosine\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"diffusion\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Training from Scratch\n",
    "The training process is implemented using a custom PyTorch function, specifying parameters such as the number of epochs and checkpoints. Various callbacks are configured to monitor and save the model during training. The training process is then initiated, logging progress and completing the model's training. Finally, the trained models are saved to the specified directory and returned for further use. This process is happening in the `train_model` function, which gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the training process.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the trained models and logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training None -> trans model from scratch\n",
      "Model params: {'num_classes': 0, 'is_y_cond': 'none', 'rtdl_params': {'d_layers': [512, 1024, 1024, 1024, 1024, 512], 'dropout': 0.0}, 'd_in': 8}\n",
      "mlp\n",
      "Step 500/200000 MLoss: 0.0 GLoss: 0.2509 Sum: 0.2509\n",
      "Step 1000/200000 MLoss: 0.0 GLoss: 0.2386 Sum: 0.2386\n",
      "Step 1500/200000 MLoss: 0.0 GLoss: 0.232 Sum: 0.232\n",
      "Step 2000/200000 MLoss: 0.0 GLoss: 0.23 Sum: 0.23\n",
      "Step 2500/200000 MLoss: 0.0 GLoss: 0.2271 Sum: 0.2271\n",
      "Step 3000/200000 MLoss: 0.0 GLoss: 0.2261 Sum: 0.2261\n",
      "Step 3500/200000 MLoss: 0.0 GLoss: 0.2244 Sum: 0.2244\n",
      "Step 4000/200000 MLoss: 0.0 GLoss: 0.2252 Sum: 0.2252\n",
      "Step 4500/200000 MLoss: 0.0 GLoss: 0.2243 Sum: 0.2243\n",
      "Step 5000/200000 MLoss: 0.0 GLoss: 0.2241 Sum: 0.2241\n",
      "Step 5500/200000 MLoss: 0.0 GLoss: 0.2235 Sum: 0.2235\n",
      "Step 6000/200000 MLoss: 0.0 GLoss: 0.2211 Sum: 0.2211\n",
      "Step 6500/200000 MLoss: 0.0 GLoss: 0.2221 Sum: 0.2221\n",
      "Step 7000/200000 MLoss: 0.0 GLoss: 0.2222 Sum: 0.2222\n",
      "Step 7500/200000 MLoss: 0.0 GLoss: 0.2224 Sum: 0.2224\n",
      "Step 8000/200000 MLoss: 0.0 GLoss: 0.2216 Sum: 0.2216\n",
      "Step 8500/200000 MLoss: 0.0 GLoss: 0.2218 Sum: 0.2218\n",
      "Step 9000/200000 MLoss: 0.0 GLoss: 0.2198 Sum: 0.2198\n",
      "Step 9500/200000 MLoss: 0.0 GLoss: 0.2195 Sum: 0.2195\n",
      "Step 10000/200000 MLoss: 0.0 GLoss: 0.2213 Sum: 0.2213\n",
      "Step 10500/200000 MLoss: 0.0 GLoss: 0.2192 Sum: 0.2192\n",
      "Step 11000/200000 MLoss: 0.0 GLoss: 0.22 Sum: 0.22\n",
      "Step 11500/200000 MLoss: 0.0 GLoss: 0.2192 Sum: 0.2192\n",
      "Step 12000/200000 MLoss: 0.0 GLoss: 0.2198 Sum: 0.2198\n",
      "Step 12500/200000 MLoss: 0.0 GLoss: 0.2187 Sum: 0.2187\n",
      "Step 13000/200000 MLoss: 0.0 GLoss: 0.2188 Sum: 0.2188\n",
      "Step 13500/200000 MLoss: 0.0 GLoss: 0.2185 Sum: 0.2185\n",
      "Step 14000/200000 MLoss: 0.0 GLoss: 0.2182 Sum: 0.2182\n",
      "Step 14500/200000 MLoss: 0.0 GLoss: 0.2176 Sum: 0.2176\n",
      "Step 15000/200000 MLoss: 0.0 GLoss: 0.2181 Sum: 0.2181\n",
      "Step 15500/200000 MLoss: 0.0 GLoss: 0.2178 Sum: 0.2178\n",
      "Step 16000/200000 MLoss: 0.0 GLoss: 0.2179 Sum: 0.2179\n",
      "Step 16500/200000 MLoss: 0.0 GLoss: 0.2174 Sum: 0.2174\n",
      "Step 17000/200000 MLoss: 0.0 GLoss: 0.2178 Sum: 0.2178\n",
      "Step 17500/200000 MLoss: 0.0 GLoss: 0.2177 Sum: 0.2177\n",
      "Step 18000/200000 MLoss: 0.0 GLoss: 0.2166 Sum: 0.2166\n",
      "Step 18500/200000 MLoss: 0.0 GLoss: 0.217 Sum: 0.217\n",
      "Step 19000/200000 MLoss: 0.0 GLoss: 0.2167 Sum: 0.2167\n",
      "Step 19500/200000 MLoss: 0.0 GLoss: 0.2166 Sum: 0.2166\n",
      "Step 20000/200000 MLoss: 0.0 GLoss: 0.2163 Sum: 0.2163\n",
      "Step 20500/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 21000/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 21500/200000 MLoss: 0.0 GLoss: 0.2158 Sum: 0.2158\n",
      "Step 22000/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 22500/200000 MLoss: 0.0 GLoss: 0.2158 Sum: 0.2158\n",
      "Step 23000/200000 MLoss: 0.0 GLoss: 0.2165 Sum: 0.2165\n",
      "Step 23500/200000 MLoss: 0.0 GLoss: 0.2155 Sum: 0.2155\n",
      "Step 24000/200000 MLoss: 0.0 GLoss: 0.2149 Sum: 0.2149\n",
      "Step 24500/200000 MLoss: 0.0 GLoss: 0.2151 Sum: 0.2151\n",
      "Step 25000/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 25500/200000 MLoss: 0.0 GLoss: 0.2147 Sum: 0.2147\n",
      "Step 26000/200000 MLoss: 0.0 GLoss: 0.2156 Sum: 0.2156\n",
      "Step 26500/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 27000/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 27500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 28000/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 28500/200000 MLoss: 0.0 GLoss: 0.2137 Sum: 0.2137\n",
      "Step 29000/200000 MLoss: 0.0 GLoss: 0.2145 Sum: 0.2145\n",
      "Step 29500/200000 MLoss: 0.0 GLoss: 0.2146 Sum: 0.2146\n",
      "Step 30000/200000 MLoss: 0.0 GLoss: 0.214 Sum: 0.214\n",
      "Step 30500/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 31000/200000 MLoss: 0.0 GLoss: 0.2141 Sum: 0.2141\n",
      "Step 31500/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 32000/200000 MLoss: 0.0 GLoss: 0.2129 Sum: 0.2129\n",
      "Step 32500/200000 MLoss: 0.0 GLoss: 0.2143 Sum: 0.2143\n",
      "Step 33000/200000 MLoss: 0.0 GLoss: 0.2132 Sum: 0.2132\n",
      "Step 33500/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 34000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 34500/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 35000/200000 MLoss: 0.0 GLoss: 0.2131 Sum: 0.2131\n",
      "Step 35500/200000 MLoss: 0.0 GLoss: 0.2138 Sum: 0.2138\n",
      "Step 36000/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 36500/200000 MLoss: 0.0 GLoss: 0.2128 Sum: 0.2128\n",
      "Step 37000/200000 MLoss: 0.0 GLoss: 0.2127 Sum: 0.2127\n",
      "Step 37500/200000 MLoss: 0.0 GLoss: 0.2123 Sum: 0.2123\n",
      "Step 38000/200000 MLoss: 0.0 GLoss: 0.2134 Sum: 0.2134\n",
      "Step 38500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 39000/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 39500/200000 MLoss: 0.0 GLoss: 0.2133 Sum: 0.2133\n",
      "Step 40000/200000 MLoss: 0.0 GLoss: 0.2118 Sum: 0.2118\n",
      "Step 40500/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 41000/200000 MLoss: 0.0 GLoss: 0.2125 Sum: 0.2125\n",
      "Step 41500/200000 MLoss: 0.0 GLoss: 0.2124 Sum: 0.2124\n",
      "Step 42000/200000 MLoss: 0.0 GLoss: 0.2116 Sum: 0.2116\n",
      "Step 42500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 43000/200000 MLoss: 0.0 GLoss: 0.2112 Sum: 0.2112\n",
      "Step 43500/200000 MLoss: 0.0 GLoss: 0.212 Sum: 0.212\n",
      "Step 44000/200000 MLoss: 0.0 GLoss: 0.2126 Sum: 0.2126\n",
      "Step 44500/200000 MLoss: 0.0 GLoss: 0.211 Sum: 0.211\n",
      "Step 45000/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 45500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 46000/200000 MLoss: 0.0 GLoss: 0.2111 Sum: 0.2111\n",
      "Step 46500/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 47000/200000 MLoss: 0.0 GLoss: 0.2113 Sum: 0.2113\n",
      "Step 47500/200000 MLoss: 0.0 GLoss: 0.2119 Sum: 0.2119\n",
      "Step 48000/200000 MLoss: 0.0 GLoss: 0.2114 Sum: 0.2114\n",
      "Step 48500/200000 MLoss: 0.0 GLoss: 0.2109 Sum: 0.2109\n",
      "Step 49000/200000 MLoss: 0.0 GLoss: 0.2113 Sum: 0.2113\n",
      "Step 49500/200000 MLoss: 0.0 GLoss: 0.2117 Sum: 0.2117\n",
      "Step 50000/200000 MLoss: 0.0 GLoss: 0.2107 Sum: 0.2107\n",
      "Step 50500/200000 MLoss: 0.0 GLoss: 0.2115 Sum: 0.2115\n",
      "Step 51000/200000 MLoss: 0.0 GLoss: 0.2106 Sum: 0.2106\n",
      "Step 51500/200000 MLoss: 0.0 GLoss: 0.2104 Sum: 0.2104\n",
      "Step 52000/200000 MLoss: 0.0 GLoss: 0.2102 Sum: 0.2102\n",
      "Step 52500/200000 MLoss: 0.0 GLoss: 0.2101 Sum: 0.2101\n",
      "Step 53000/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 53500/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 54000/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 54500/200000 MLoss: 0.0 GLoss: 0.2103 Sum: 0.2103\n",
      "Step 55000/200000 MLoss: 0.0 GLoss: 0.2104 Sum: 0.2104\n",
      "Step 55500/200000 MLoss: 0.0 GLoss: 0.2103 Sum: 0.2103\n",
      "Step 56000/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 56500/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 57000/200000 MLoss: 0.0 GLoss: 0.21 Sum: 0.21\n",
      "Step 57500/200000 MLoss: 0.0 GLoss: 0.2105 Sum: 0.2105\n",
      "Step 58000/200000 MLoss: 0.0 GLoss: 0.2108 Sum: 0.2108\n",
      "Step 58500/200000 MLoss: 0.0 GLoss: 0.2102 Sum: 0.2102\n",
      "Step 59000/200000 MLoss: 0.0 GLoss: 0.2097 Sum: 0.2097\n",
      "Step 59500/200000 MLoss: 0.0 GLoss: 0.2093 Sum: 0.2093\n",
      "Step 60000/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 60500/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 61000/200000 MLoss: 0.0 GLoss: 0.2093 Sum: 0.2093\n",
      "Step 61500/200000 MLoss: 0.0 GLoss: 0.2081 Sum: 0.2081\n",
      "Step 62000/200000 MLoss: 0.0 GLoss: 0.2094 Sum: 0.2094\n",
      "Step 62500/200000 MLoss: 0.0 GLoss: 0.2096 Sum: 0.2096\n",
      "Step 63000/200000 MLoss: 0.0 GLoss: 0.2098 Sum: 0.2098\n",
      "Step 63500/200000 MLoss: 0.0 GLoss: 0.2089 Sum: 0.2089\n",
      "Step 64000/200000 MLoss: 0.0 GLoss: 0.2092 Sum: 0.2092\n",
      "Step 64500/200000 MLoss: 0.0 GLoss: 0.2095 Sum: 0.2095\n",
      "Step 65000/200000 MLoss: 0.0 GLoss: 0.2089 Sum: 0.2089\n",
      "Step 65500/200000 MLoss: 0.0 GLoss: 0.2088 Sum: 0.2088\n",
      "Step 66000/200000 MLoss: 0.0 GLoss: 0.2094 Sum: 0.2094\n",
      "Step 66500/200000 MLoss: 0.0 GLoss: 0.2099 Sum: 0.2099\n",
      "Step 67000/200000 MLoss: 0.0 GLoss: 0.2087 Sum: 0.2087\n",
      "Step 67500/200000 MLoss: 0.0 GLoss: 0.2085 Sum: 0.2085\n",
      "Step 68000/200000 MLoss: 0.0 GLoss: 0.209 Sum: 0.209\n",
      "Step 68500/200000 MLoss: 0.0 GLoss: 0.2088 Sum: 0.2088\n",
      "Step 69000/200000 MLoss: 0.0 GLoss: 0.2081 Sum: 0.2081\n",
      "Step 69500/200000 MLoss: 0.0 GLoss: 0.209 Sum: 0.209\n",
      "Step 70000/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 70500/200000 MLoss: 0.0 GLoss: 0.2086 Sum: 0.2086\n",
      "Step 71000/200000 MLoss: 0.0 GLoss: 0.2089 Sum: 0.2089\n",
      "Step 71500/200000 MLoss: 0.0 GLoss: 0.2087 Sum: 0.2087\n",
      "Step 72000/200000 MLoss: 0.0 GLoss: 0.2091 Sum: 0.2091\n",
      "Step 72500/200000 MLoss: 0.0 GLoss: 0.2085 Sum: 0.2085\n",
      "Step 73000/200000 MLoss: 0.0 GLoss: 0.2082 Sum: 0.2082\n",
      "Step 73500/200000 MLoss: 0.0 GLoss: 0.2083 Sum: 0.2083\n",
      "Step 74000/200000 MLoss: 0.0 GLoss: 0.2081 Sum: 0.2081\n",
      "Step 74500/200000 MLoss: 0.0 GLoss: 0.2078 Sum: 0.2078\n",
      "Step 75000/200000 MLoss: 0.0 GLoss: 0.2072 Sum: 0.2072\n",
      "Step 75500/200000 MLoss: 0.0 GLoss: 0.2085 Sum: 0.2085\n",
      "Step 76000/200000 MLoss: 0.0 GLoss: 0.2084 Sum: 0.2084\n",
      "Step 76500/200000 MLoss: 0.0 GLoss: 0.2083 Sum: 0.2083\n",
      "Step 77000/200000 MLoss: 0.0 GLoss: 0.2082 Sum: 0.2082\n",
      "Step 77500/200000 MLoss: 0.0 GLoss: 0.2082 Sum: 0.2082\n",
      "Step 78000/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 78500/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 79000/200000 MLoss: 0.0 GLoss: 0.2072 Sum: 0.2072\n",
      "Step 79500/200000 MLoss: 0.0 GLoss: 0.2077 Sum: 0.2077\n",
      "Step 80000/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 80500/200000 MLoss: 0.0 GLoss: 0.2077 Sum: 0.2077\n",
      "Step 81000/200000 MLoss: 0.0 GLoss: 0.2081 Sum: 0.2081\n",
      "Step 81500/200000 MLoss: 0.0 GLoss: 0.2071 Sum: 0.2071\n",
      "Step 82000/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 82500/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 83000/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 83500/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 84000/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 84500/200000 MLoss: 0.0 GLoss: 0.2077 Sum: 0.2077\n",
      "Step 85000/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 85500/200000 MLoss: 0.0 GLoss: 0.2079 Sum: 0.2079\n",
      "Step 86000/200000 MLoss: 0.0 GLoss: 0.2072 Sum: 0.2072\n",
      "Step 86500/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 87000/200000 MLoss: 0.0 GLoss: 0.2081 Sum: 0.2081\n",
      "Step 87500/200000 MLoss: 0.0 GLoss: 0.2067 Sum: 0.2067\n",
      "Step 88000/200000 MLoss: 0.0 GLoss: 0.2075 Sum: 0.2075\n",
      "Step 88500/200000 MLoss: 0.0 GLoss: 0.2068 Sum: 0.2068\n",
      "Step 89000/200000 MLoss: 0.0 GLoss: 0.2069 Sum: 0.2069\n",
      "Step 89500/200000 MLoss: 0.0 GLoss: 0.2066 Sum: 0.2066\n",
      "Step 90000/200000 MLoss: 0.0 GLoss: 0.2074 Sum: 0.2074\n",
      "Step 90500/200000 MLoss: 0.0 GLoss: 0.207 Sum: 0.207\n",
      "Step 91000/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 91500/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 92000/200000 MLoss: 0.0 GLoss: 0.2073 Sum: 0.2073\n",
      "Step 92500/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 93000/200000 MLoss: 0.0 GLoss: 0.2065 Sum: 0.2065\n",
      "Step 93500/200000 MLoss: 0.0 GLoss: 0.2072 Sum: 0.2072\n",
      "Step 94000/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 94500/200000 MLoss: 0.0 GLoss: 0.2059 Sum: 0.2059\n",
      "Step 95000/200000 MLoss: 0.0 GLoss: 0.207 Sum: 0.207\n",
      "Step 95500/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 96000/200000 MLoss: 0.0 GLoss: 0.2059 Sum: 0.2059\n",
      "Step 96500/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 97000/200000 MLoss: 0.0 GLoss: 0.2076 Sum: 0.2076\n",
      "Step 97500/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 98000/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 98500/200000 MLoss: 0.0 GLoss: 0.2058 Sum: 0.2058\n",
      "Step 99000/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 99500/200000 MLoss: 0.0 GLoss: 0.2072 Sum: 0.2072\n",
      "Step 100000/200000 MLoss: 0.0 GLoss: 0.2066 Sum: 0.2066\n",
      "Step 100500/200000 MLoss: 0.0 GLoss: 0.2069 Sum: 0.2069\n",
      "Step 101000/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 101500/200000 MLoss: 0.0 GLoss: 0.2066 Sum: 0.2066\n",
      "Step 102000/200000 MLoss: 0.0 GLoss: 0.2063 Sum: 0.2063\n",
      "Step 102500/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 103000/200000 MLoss: 0.0 GLoss: 0.2055 Sum: 0.2055\n",
      "Step 103500/200000 MLoss: 0.0 GLoss: 0.2066 Sum: 0.2066\n",
      "Step 104000/200000 MLoss: 0.0 GLoss: 0.2059 Sum: 0.2059\n",
      "Step 104500/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 105000/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 105500/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 106000/200000 MLoss: 0.0 GLoss: 0.2055 Sum: 0.2055\n",
      "Step 106500/200000 MLoss: 0.0 GLoss: 0.2067 Sum: 0.2067\n",
      "Step 107000/200000 MLoss: 0.0 GLoss: 0.2051 Sum: 0.2051\n",
      "Step 107500/200000 MLoss: 0.0 GLoss: 0.205 Sum: 0.205\n",
      "Step 108000/200000 MLoss: 0.0 GLoss: 0.2055 Sum: 0.2055\n",
      "Step 108500/200000 MLoss: 0.0 GLoss: 0.206 Sum: 0.206\n",
      "Step 109000/200000 MLoss: 0.0 GLoss: 0.2056 Sum: 0.2056\n",
      "Step 109500/200000 MLoss: 0.0 GLoss: 0.205 Sum: 0.205\n",
      "Step 110000/200000 MLoss: 0.0 GLoss: 0.205 Sum: 0.205\n",
      "Step 110500/200000 MLoss: 0.0 GLoss: 0.2044 Sum: 0.2044\n",
      "Step 111000/200000 MLoss: 0.0 GLoss: 0.2048 Sum: 0.2048\n",
      "Step 111500/200000 MLoss: 0.0 GLoss: 0.2046 Sum: 0.2046\n",
      "Step 112000/200000 MLoss: 0.0 GLoss: 0.2061 Sum: 0.2061\n",
      "Step 112500/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 113000/200000 MLoss: 0.0 GLoss: 0.2055 Sum: 0.2055\n",
      "Step 113500/200000 MLoss: 0.0 GLoss: 0.2054 Sum: 0.2054\n",
      "Step 114000/200000 MLoss: 0.0 GLoss: 0.2057 Sum: 0.2057\n",
      "Step 114500/200000 MLoss: 0.0 GLoss: 0.205 Sum: 0.205\n",
      "Step 115000/200000 MLoss: 0.0 GLoss: 0.2049 Sum: 0.2049\n",
      "Step 115500/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 116000/200000 MLoss: 0.0 GLoss: 0.2057 Sum: 0.2057\n",
      "Step 116500/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 117000/200000 MLoss: 0.0 GLoss: 0.2052 Sum: 0.2052\n",
      "Step 117500/200000 MLoss: 0.0 GLoss: 0.2059 Sum: 0.2059\n",
      "Step 118000/200000 MLoss: 0.0 GLoss: 0.2046 Sum: 0.2046\n",
      "Step 118500/200000 MLoss: 0.0 GLoss: 0.2046 Sum: 0.2046\n",
      "Step 119000/200000 MLoss: 0.0 GLoss: 0.2054 Sum: 0.2054\n",
      "Step 119500/200000 MLoss: 0.0 GLoss: 0.2045 Sum: 0.2045\n",
      "Step 120000/200000 MLoss: 0.0 GLoss: 0.2051 Sum: 0.2051\n",
      "Step 120500/200000 MLoss: 0.0 GLoss: 0.2049 Sum: 0.2049\n",
      "Step 121000/200000 MLoss: 0.0 GLoss: 0.2044 Sum: 0.2044\n",
      "Step 121500/200000 MLoss: 0.0 GLoss: 0.2045 Sum: 0.2045\n",
      "Step 122000/200000 MLoss: 0.0 GLoss: 0.2049 Sum: 0.2049\n",
      "Step 122500/200000 MLoss: 0.0 GLoss: 0.204 Sum: 0.204\n",
      "Step 123000/200000 MLoss: 0.0 GLoss: 0.2047 Sum: 0.2047\n",
      "Step 123500/200000 MLoss: 0.0 GLoss: 0.204 Sum: 0.204\n",
      "Step 124000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 124500/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 125000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 125500/200000 MLoss: 0.0 GLoss: 0.2047 Sum: 0.2047\n",
      "Step 126000/200000 MLoss: 0.0 GLoss: 0.204 Sum: 0.204\n",
      "Step 126500/200000 MLoss: 0.0 GLoss: 0.2045 Sum: 0.2045\n",
      "Step 127000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 127500/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 128000/200000 MLoss: 0.0 GLoss: 0.2039 Sum: 0.2039\n",
      "Step 128500/200000 MLoss: 0.0 GLoss: 0.2042 Sum: 0.2042\n",
      "Step 129000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 129500/200000 MLoss: 0.0 GLoss: 0.2046 Sum: 0.2046\n",
      "Step 130000/200000 MLoss: 0.0 GLoss: 0.2036 Sum: 0.2036\n",
      "Step 130500/200000 MLoss: 0.0 GLoss: 0.2053 Sum: 0.2053\n",
      "Step 131000/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 131500/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 132000/200000 MLoss: 0.0 GLoss: 0.2042 Sum: 0.2042\n",
      "Step 132500/200000 MLoss: 0.0 GLoss: 0.204 Sum: 0.204\n",
      "Step 133000/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 133500/200000 MLoss: 0.0 GLoss: 0.205 Sum: 0.205\n",
      "Step 134000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 134500/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 135000/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 135500/200000 MLoss: 0.0 GLoss: 0.2042 Sum: 0.2042\n",
      "Step 136000/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 136500/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 137000/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 137500/200000 MLoss: 0.0 GLoss: 0.204 Sum: 0.204\n",
      "Step 138000/200000 MLoss: 0.0 GLoss: 0.2043 Sum: 0.2043\n",
      "Step 138500/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 139000/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 139500/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 140000/200000 MLoss: 0.0 GLoss: 0.2031 Sum: 0.2031\n",
      "Step 140500/200000 MLoss: 0.0 GLoss: 0.2032 Sum: 0.2032\n",
      "Step 141000/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 141500/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 142000/200000 MLoss: 0.0 GLoss: 0.2028 Sum: 0.2028\n",
      "Step 142500/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 143000/200000 MLoss: 0.0 GLoss: 0.2039 Sum: 0.2039\n",
      "Step 143500/200000 MLoss: 0.0 GLoss: 0.2034 Sum: 0.2034\n",
      "Step 144000/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 144500/200000 MLoss: 0.0 GLoss: 0.2036 Sum: 0.2036\n",
      "Step 145000/200000 MLoss: 0.0 GLoss: 0.2041 Sum: 0.2041\n",
      "Step 145500/200000 MLoss: 0.0 GLoss: 0.2032 Sum: 0.2032\n",
      "Step 146000/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 146500/200000 MLoss: 0.0 GLoss: 0.2036 Sum: 0.2036\n",
      "Step 147000/200000 MLoss: 0.0 GLoss: 0.2039 Sum: 0.2039\n",
      "Step 147500/200000 MLoss: 0.0 GLoss: 0.2026 Sum: 0.2026\n",
      "Step 148000/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 148500/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 149000/200000 MLoss: 0.0 GLoss: 0.2038 Sum: 0.2038\n",
      "Step 149500/200000 MLoss: 0.0 GLoss: 0.2024 Sum: 0.2024\n",
      "Step 150000/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 150500/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 151000/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 151500/200000 MLoss: 0.0 GLoss: 0.2037 Sum: 0.2037\n",
      "Step 152000/200000 MLoss: 0.0 GLoss: 0.2033 Sum: 0.2033\n",
      "Step 152500/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 153000/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 153500/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 154000/200000 MLoss: 0.0 GLoss: 0.2035 Sum: 0.2035\n",
      "Step 154500/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 155000/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 155500/200000 MLoss: 0.0 GLoss: 0.2024 Sum: 0.2024\n",
      "Step 156000/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 156500/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 157000/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 157500/200000 MLoss: 0.0 GLoss: 0.2039 Sum: 0.2039\n",
      "Step 158000/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 158500/200000 MLoss: 0.0 GLoss: 0.2026 Sum: 0.2026\n",
      "Step 159000/200000 MLoss: 0.0 GLoss: 0.202 Sum: 0.202\n",
      "Step 159500/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 160000/200000 MLoss: 0.0 GLoss: 0.2024 Sum: 0.2024\n",
      "Step 160500/200000 MLoss: 0.0 GLoss: 0.2022 Sum: 0.2022\n",
      "Step 161000/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 161500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 162000/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 162500/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 163000/200000 MLoss: 0.0 GLoss: 0.2022 Sum: 0.2022\n",
      "Step 163500/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 164000/200000 MLoss: 0.0 GLoss: 0.2026 Sum: 0.2026\n",
      "Step 164500/200000 MLoss: 0.0 GLoss: 0.202 Sum: 0.202\n",
      "Step 165000/200000 MLoss: 0.0 GLoss: 0.2028 Sum: 0.2028\n",
      "Step 165500/200000 MLoss: 0.0 GLoss: 0.2027 Sum: 0.2027\n",
      "Step 166000/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 166500/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 167000/200000 MLoss: 0.0 GLoss: 0.2029 Sum: 0.2029\n",
      "Step 167500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 168000/200000 MLoss: 0.0 GLoss: 0.2026 Sum: 0.2026\n",
      "Step 168500/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 169000/200000 MLoss: 0.0 GLoss: 0.202 Sum: 0.202\n",
      "Step 169500/200000 MLoss: 0.0 GLoss: 0.2025 Sum: 0.2025\n",
      "Step 170000/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 170500/200000 MLoss: 0.0 GLoss: 0.2022 Sum: 0.2022\n",
      "Step 171000/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 171500/200000 MLoss: 0.0 GLoss: 0.203 Sum: 0.203\n",
      "Step 172000/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 172500/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 173000/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 173500/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 174000/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 174500/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 175000/200000 MLoss: 0.0 GLoss: 0.2014 Sum: 0.2014\n",
      "Step 175500/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 176000/200000 MLoss: 0.0 GLoss: 0.2022 Sum: 0.2022\n",
      "Step 176500/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 177000/200000 MLoss: 0.0 GLoss: 0.2019 Sum: 0.2019\n",
      "Step 177500/200000 MLoss: 0.0 GLoss: 0.2012 Sum: 0.2012\n",
      "Step 178000/200000 MLoss: 0.0 GLoss: 0.2014 Sum: 0.2014\n",
      "Step 178500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 179000/200000 MLoss: 0.0 GLoss: 0.2011 Sum: 0.2011\n",
      "Step 179500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 180000/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 180500/200000 MLoss: 0.0 GLoss: 0.2021 Sum: 0.2021\n",
      "Step 181000/200000 MLoss: 0.0 GLoss: 0.201 Sum: 0.201\n",
      "Step 181500/200000 MLoss: 0.0 GLoss: 0.2015 Sum: 0.2015\n",
      "Step 182000/200000 MLoss: 0.0 GLoss: 0.2013 Sum: 0.2013\n",
      "Step 182500/200000 MLoss: 0.0 GLoss: 0.2012 Sum: 0.2012\n",
      "Step 183000/200000 MLoss: 0.0 GLoss: 0.201 Sum: 0.201\n",
      "Step 183500/200000 MLoss: 0.0 GLoss: 0.2012 Sum: 0.2012\n",
      "Step 184000/200000 MLoss: 0.0 GLoss: 0.201 Sum: 0.201\n",
      "Step 184500/200000 MLoss: 0.0 GLoss: 0.2014 Sum: 0.2014\n",
      "Step 185000/200000 MLoss: 0.0 GLoss: 0.2013 Sum: 0.2013\n",
      "Step 185500/200000 MLoss: 0.0 GLoss: 0.2009 Sum: 0.2009\n",
      "Step 186000/200000 MLoss: 0.0 GLoss: 0.2016 Sum: 0.2016\n",
      "Step 186500/200000 MLoss: 0.0 GLoss: 0.2015 Sum: 0.2015\n",
      "Step 187000/200000 MLoss: 0.0 GLoss: 0.2007 Sum: 0.2007\n",
      "Step 187500/200000 MLoss: 0.0 GLoss: 0.2009 Sum: 0.2009\n",
      "Step 188000/200000 MLoss: 0.0 GLoss: 0.2012 Sum: 0.2012\n",
      "Step 188500/200000 MLoss: 0.0 GLoss: 0.2009 Sum: 0.2009\n",
      "Step 189000/200000 MLoss: 0.0 GLoss: 0.2009 Sum: 0.2009\n",
      "Step 189500/200000 MLoss: 0.0 GLoss: 0.2012 Sum: 0.2012\n",
      "Step 190000/200000 MLoss: 0.0 GLoss: 0.2017 Sum: 0.2017\n",
      "Step 190500/200000 MLoss: 0.0 GLoss: 0.2009 Sum: 0.2009\n",
      "Step 191000/200000 MLoss: 0.0 GLoss: 0.2014 Sum: 0.2014\n",
      "Step 191500/200000 MLoss: 0.0 GLoss: 0.2012 Sum: 0.2012\n",
      "Step 192000/200000 MLoss: 0.0 GLoss: 0.2008 Sum: 0.2008\n",
      "Step 192500/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 193000/200000 MLoss: 0.0 GLoss: 0.2011 Sum: 0.2011\n",
      "Step 193500/200000 MLoss: 0.0 GLoss: 0.2011 Sum: 0.2011\n",
      "Step 194000/200000 MLoss: 0.0 GLoss: 0.2009 Sum: 0.2009\n",
      "Step 194500/200000 MLoss: 0.0 GLoss: 0.2018 Sum: 0.2018\n",
      "Step 195000/200000 MLoss: 0.0 GLoss: 0.2012 Sum: 0.2012\n",
      "Step 195500/200000 MLoss: 0.0 GLoss: 0.2005 Sum: 0.2005\n",
      "Step 196000/200000 MLoss: 0.0 GLoss: 0.2005 Sum: 0.2005\n",
      "Step 196500/200000 MLoss: 0.0 GLoss: 0.201 Sum: 0.201\n",
      "Step 197000/200000 MLoss: 0.0 GLoss: 0.2011 Sum: 0.2011\n",
      "Step 197500/200000 MLoss: 0.0 GLoss: 0.2008 Sum: 0.2008\n",
      "Step 198000/200000 MLoss: 0.0 GLoss: 0.2006 Sum: 0.2006\n",
      "Step 198500/200000 MLoss: 0.0 GLoss: 0.2006 Sum: 0.2006\n",
      "Step 199000/200000 MLoss: 0.0 GLoss: 0.2006 Sum: 0.2006\n",
      "Step 199500/200000 MLoss: 0.0 GLoss: 0.2013 Sum: 0.2013\n",
      "Step 200000/200000 MLoss: 0.0 GLoss: 0.2006 Sum: 0.2006\n"
     ]
    }
   ],
   "source": [
    "# Launch training from scratch\n",
    "models = clava_training(tables, relation_order, save_dir, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Pretrained Models\n",
    "If the training process from scratch takes too long, please run the following command to load pre-trained models and samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work4/xiaoyuwu/MIDSTModelsMIA/starter_kits/tabddpm_white_box/train/tabddpm_1/workspace/train_1/models/None_trans_ckpt.pkl\n",
      "None -> trans checkpoint found, loading...\n"
     ]
    }
   ],
   "source": [
    "# Use the pre-trained models\n",
    "## save_dir was determined when loading the config file\n",
    "models = clava_load_pretrained(relation_order, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Sampling\n",
    "\n",
    "Important parameters for the sampling process include:\n",
    "- `batch_size`: Mini-batch size for sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== We show the important sampling parameters below ====================\n",
      "batch_size: 20000\n",
      "classifier_scale: 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display important sampling parameters\n",
    "params_sampling = configs[\"sampling\"]\n",
    "print(\n",
    "    \"{} We show the important sampling parameters below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    ")\n",
    "for key, val in params_sampling.items():\n",
    "    print(f\"{key}: {val}\")\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data from Scratch\n",
    "To generate synthetic data from scratch, we run the following code cell. This `clava_synthesizing` function gets the following inputs:\n",
    "\n",
    "- `tables`: the relational tables with data augmentation.\n",
    "- `relation_order`: the parent-child relationships between tables.\n",
    "- `save_dir`: the directory to save the synthetic data.\n",
    "- `all_group_lengths_prob_dicts`: a dictionary that computes group size distributions for each table, used in the sampling stage to determine the size of the tables to generate.\n",
    "- `models`: the trained diffusion models.\n",
    "- `configs`: the configuration dictionary with hyperparameters and settings for the sampling process.\n",
    "- `sample_scale`: the scale factor for the sampling process.\n",
    "\n",
    "The synthetic data will be saved in the specified output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating None -> trans\n",
      "Sample size: 20000\n",
      "Sample timestep    0\n"
     ]
    }
   ],
   "source": [
    "# Generate synthetic data from scratch\n",
    "cleaned_tables, synthesizing_time_spent, matching_time_spent = clava_synthesizing(\n",
    "    tables,\n",
    "    relation_order,\n",
    "    save_dir,\n",
    "    all_group_lengths_prob_dicts,\n",
    "    models,\n",
    "    configs,\n",
    "    sample_scale=1 if \"debug\" not in configs else configs[\"debug\"][\"sample_scale\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, as some integer values are saved as strings during this process, we convert them back to integers for further evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast int values that saved as string to int for further evaluation\n",
    "for key in cleaned_tables.keys():\n",
    "    for col in cleaned_tables[key].columns:\n",
    "        if cleaned_tables[key][col].dtype == \"object\":\n",
    "            try:\n",
    "                cleaned_tables[key][col] = cleaned_tables[key][col].astype(int)\n",
    "            except ValueError:\n",
    "                print(f\"Column {col} cannot be converted to int.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- # Prepare the synthetic data and reference data for single-table metric evaluation\n",
    "shutil.copy(os.path.join(configs['general']['data_dir'], 'dataset_meta.json'), os.path.join(save_dir, 'dataset_meta.json'))\n",
    "for table_name in tables.keys():\n",
    "    shutil.copy(os.path.join(save_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "    # uncomment and run the following line if you want to use the pre-synthesized data\n",
    "    # shutil.copy(os.path.join(pretrained_dir, table_name, '_final', f'{table_name}_synthetic.csv'), os.path.join(save_dir, f'{table_name}.csv'))\n",
    "\n",
    "    shutil.copy(os.path.join(configs['general']['data_dir'], f'{table_name}_domain.json'), os.path.join(save_dir, f'{table_name}_domain.json'))\n",
    "\n",
    "test_tables, _, _ = load_multi_table(save_dir, verbose=False)\n",
    "real_tables, _, _ = load_multi_table(configs['general']['data_dir'], verbose=False)\n",
    "\n",
    "# Single table metrics\n",
    "for table_name in tables.keys():\n",
    "    print(f'Generating report for {table_name}')\n",
    "    real_data = real_tables[table_name]['df']\n",
    "    syn_data = cleaned_tables[table_name]\n",
    "    domain_dict = real_tables[table_name]['domain']\n",
    "\n",
    "    if configs['general']['workspace_dir'] is not None:\n",
    "        test_data = test_tables[table_name]['df']\n",
    "    else:\n",
    "        test_data = None\n",
    "\n",
    "    gen_single_report(\n",
    "        real_data, \n",
    "        syn_data,\n",
    "        domain_dict,\n",
    "        table_name,\n",
    "        save_dir,\n",
    "        alpha_beta_sample_size=200_000,\n",
    "        test_data=test_data\n",
    "    ) -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "**Pang, Wei, et al.** \"ClavaDDPM: Multi-relational Data Synthesis with Cluster-guided Diffusion Models.\" *preprint* (2024).\n",
    "\n",
    "**GitHub Repository:** [ClavaDDPM](https://github.com/weipang142857/ClavaDDPM)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midst-models-9WC0e6Lb-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
