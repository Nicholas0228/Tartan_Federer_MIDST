{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from midst_models.single_table_TabDDPM.lib import Dataset, TaskType, transform_dataset\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union, cast\n",
    "from pathlib import Path\n",
    "from dataclasses import astuple, dataclass, replace\n",
    "from midst_models.single_table_TabDDPM.lib import (\n",
    "    Transformations,\n",
    "    prepare_fast_dataloader,\n",
    "    round_columns,\n",
    ")\n",
    "import json\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_load_pretrained_customized,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_utils import load_multi_table_customized\n",
    "import pickle\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import CustomUnpickler\n",
    "import os\n",
    "\n",
    "\n",
    "CAT_MISSING_VALUE = \"__nan__\"\n",
    "CAT_RARE_VALUE = \"__rare__\"\n",
    "Normalization = Literal[\"standard\", \"quantile\", \"minmax\"]\n",
    "NumNanPolicy = Literal[\"drop-rows\", \"mean\"]\n",
    "CatNanPolicy = Literal[\"most_frequent\"]\n",
    "CatEncoding = Literal[\"one-hot\", \"counter\"]\n",
    "YPolicy = Literal[\"default\"]\n",
    "ArrayDict = Dict[str, np.ndarray]\n",
    "\n",
    "def raise_unknown(unknown_what: str, unknown_value: Any):\n",
    "    raise ValueError(f\"Unknown {unknown_what}: {unknown_value}\")\n",
    "\n",
    "\n",
    "def get_table_info(df, domain_dict, y_col):\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    for col in df.columns:\n",
    "        if col in domain_dict and col != y_col:\n",
    "            if domain_dict[col][\"type\"] == \"discrete\":\n",
    "                cat_cols.append(col)\n",
    "            else:\n",
    "                num_cols.append(col)\n",
    "\n",
    "    df_info = {}\n",
    "    df_info[\"cat_cols\"] = cat_cols\n",
    "    df_info[\"num_cols\"] = num_cols\n",
    "    df_info[\"y_col\"] = y_col\n",
    "    df_info[\"n_classes\"] = 0\n",
    "    df_info[\"task_type\"] = \"multiclass\"\n",
    "\n",
    "    return df_info\n",
    "\n",
    "def get_T_dict():\n",
    "    return {\n",
    "        \"seed\": 0,\n",
    "        \"normalization\": \"quantile\",\n",
    "        \"num_nan_policy\": None,\n",
    "        \"cat_nan_policy\": None,\n",
    "        \"cat_min_frequency\": None,\n",
    "        \"cat_encoding\": None,\n",
    "        \"y_policy\": \"default\",\n",
    "    }\n",
    "\n",
    "def get_model_params(rtdl_params=None):\n",
    "    return {\n",
    "        \"num_classes\": 0,\n",
    "        \"is_y_cond\": \"none\",\n",
    "        \"rtdl_params\": {\"d_layers\": [512, 1024, 1024, 1024, 1024, 512], \"dropout\": 0.0}\n",
    "        if rtdl_params is None\n",
    "        else rtdl_params,\n",
    "    }\n",
    "\n",
    "def build_target(\n",
    "    y: ArrayDict, policy: Optional[YPolicy], task_type: TaskType\n",
    ") -> Tuple[ArrayDict, Dict[str, Any]]:\n",
    "    info: Dict[str, Any] = {\"policy\": policy}\n",
    "    if policy is None:\n",
    "        pass\n",
    "    elif policy == \"default\":\n",
    "        if task_type == TaskType.REGRESSION:\n",
    "            mean, std = float(y[\"train\"].mean()), float(y[\"train\"].std())\n",
    "            y = {k: (v - mean) / std for k, v in y.items()}\n",
    "            info[\"mean\"] = mean\n",
    "            info[\"std\"] = std\n",
    "    else:\n",
    "        raise_unknown(\"policy\", policy)\n",
    "    return y, info\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Transformations:\n",
    "    seed: int = 0\n",
    "    normalization: Optional[Normalization] = None\n",
    "    num_nan_policy: Optional[NumNanPolicy] = None\n",
    "    cat_nan_policy: Optional[CatNanPolicy] = None\n",
    "    cat_min_frequency: Optional[float] = None\n",
    "    cat_encoding: Optional[CatEncoding] = None\n",
    "    y_policy: Optional[YPolicy] = \"default\"\n",
    "\n",
    "\n",
    "def transform_dataset(\n",
    "    dataset: Dataset,\n",
    "    transformations: Transformations,\n",
    "    cache_dir: Optional[Path],\n",
    "    transform_cols_num: int = 0,\n",
    "    normalizer=None,\n",
    "    cat_transform=None, \n",
    "    num_transform=None\n",
    ") -> Dataset:\n",
    "    # WARNING: the order of transformations matters. Moreover, the current\n",
    "    # implementation is not ideal in that sense.\n",
    "    if cache_dir is not None:\n",
    "        transformations_md5 = hashlib.md5(\n",
    "            str(transformations).encode(\"utf-8\")\n",
    "        ).hexdigest()\n",
    "        transformations_str = \"__\".join(map(str, astuple(transformations)))\n",
    "        cache_path = (\n",
    "            cache_dir / f\"cache__{transformations_str}__{transformations_md5}.pickle\"\n",
    "        )\n",
    "        if cache_path.exists():\n",
    "            cache_transformations, value = util.load_pickle(cache_path)\n",
    "            if transformations == cache_transformations:\n",
    "                print(\n",
    "                    f\"Using cached features: {cache_dir.name + '/' + cache_path.name}\"\n",
    "                )\n",
    "                return value\n",
    "            else:\n",
    "                raise RuntimeError(f\"Hash collision for {cache_path}\")\n",
    "    else:\n",
    "        cache_path = None\n",
    "\n",
    "\n",
    "    cat_transform = None\n",
    "    X_num = dataset.X_num\n",
    "    X_num = {k: num_transform.transform(v) for k, v in X_num.items()}\n",
    "\n",
    "    if dataset.X_cat is None:\n",
    "        assert transformations.cat_nan_policy is None\n",
    "        assert transformations.cat_min_frequency is None\n",
    "        # assert transformations.cat_encoding is None\n",
    "        X_cat = None\n",
    "    else:\n",
    "        X_cat = cat_process_nans(dataset.X_cat, transformations.cat_nan_policy)\n",
    "        if transformations.cat_min_frequency is not None:\n",
    "            X_cat = cat_drop_rare(X_cat, transformations.cat_min_frequency)\n",
    "        \n",
    "        if cat_transform is None:\n",
    "            raise ValueError(\"See why no cat_tramsform\")\n",
    "        else:\n",
    "            X_cat = {k: cat_transform.transform(v).astype(\"float32\") for k, v in X_cat.items()} \n",
    "            X_num = (\n",
    "                X_cat\n",
    "                if X_num is None\n",
    "                else {x: np.hstack([X_num[x], X_cat[x]]) for x in X_num}\n",
    "            )\n",
    "            X_cat = None\n",
    "\n",
    "    y, y_info = build_target(dataset.y, transformations.y_policy, dataset.task_type)\n",
    "\n",
    "    dataset = replace(dataset, X_num=X_num, X_cat=X_cat, y=y, y_info=y_info)\n",
    "    dataset.num_transform = num_transform\n",
    "    dataset.cat_transform = cat_transform\n",
    "\n",
    "    return dataset\n",
    "def make_dataset_from_df_with_loaded(df, T, is_y_cond, ratios=[0.7, 0.2, 0.1], df_info=None, std=0, label_encoders=None, num_transform=None):\n",
    "\n",
    "    cat_column_orders = []\n",
    "    num_column_orders = []\n",
    "    index_to_column = list(df.columns)\n",
    "    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n",
    "\n",
    "    if df_info[\"n_classes\"] > 0:\n",
    "        X_cat = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n",
    "        X_num = {} if df_info[\"num_cols\"] is not None else None\n",
    "        y = {}\n",
    "\n",
    "        cat_cols_with_y = []\n",
    "        if df_info[\"cat_cols\"] is not None:\n",
    "            cat_cols_with_y += df_info[\"cat_cols\"]\n",
    "        if is_y_cond == \"concat\":\n",
    "            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n",
    "\n",
    "        if len(cat_cols_with_y) > 0:\n",
    "            X_cat[\"train\"] = df[cat_cols_with_y].to_numpy(dtype=np.str_)\n",
    "\n",
    "        y[\"train\"] = df[df_info[\"y_col\"]].values.astype(np.float32)\n",
    "\n",
    "        if df_info[\"num_cols\"] is not None:\n",
    "            X_num[\"train\"] = df[df_info[\"num_cols\"]].values.astype(np.float32)\n",
    "\n",
    "        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n",
    "        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n",
    "\n",
    "    else:\n",
    "        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n",
    "        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n",
    "        y = {}\n",
    "\n",
    "        num_cols_with_y = []\n",
    "        if df_info[\"num_cols\"] is not None:\n",
    "            num_cols_with_y += df_info[\"num_cols\"]\n",
    "        if is_y_cond == \"concat\":\n",
    "            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n",
    "\n",
    "        if len(num_cols_with_y) > 0:\n",
    "            X_num[\"train\"] = df[num_cols_with_y].values.astype(np.float32)\n",
    "\n",
    "        y[\"train\"] = df[df_info[\"y_col\"]].values.astype(np.float32)\n",
    "\n",
    "        if df_info[\"cat_cols\"] is not None:\n",
    "            X_cat[\"train\"] = df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)\n",
    "\n",
    "        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n",
    "        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n",
    "\n",
    "    column_orders = num_column_orders + cat_column_orders\n",
    "    column_orders = [index_to_column[index] for index in column_orders]\n",
    "\n",
    "    if X_cat is not None and len(df_info[\"cat_cols\"]) > 0:\n",
    "        X_cat_all = X_cat[\"train\"]\n",
    "        X_cat_converted = []\n",
    "        for col_index in range(X_cat_all.shape[1]):\n",
    "            if label_encoders is None:\n",
    "                raise ValueError('Should be loaded: label_encoder')\n",
    "            else:\n",
    "                print('label_encoders loaded')\n",
    "\n",
    "            X_cat_converted.append(\n",
    "                label_encoders[col_index].transform(X_cat_all[:, col_index]).astype(float)\n",
    "            )\n",
    "            if std > 0:\n",
    "                # add noise\n",
    "                X_cat_converted[-1] += np.random.normal(\n",
    "                    0, std, X_cat_converted[-1].shape\n",
    "                )\n",
    "\n",
    "\n",
    "        X_cat_converted = np.vstack(X_cat_converted).T\n",
    "\n",
    "        train_num = X_cat[\"train\"].shape[0]\n",
    "\n",
    "        X_cat[\"train\"] = X_cat_converted[:train_num, :]\n",
    "\n",
    "        if len(X_num) > 0:\n",
    "            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n",
    "        else:\n",
    "            X_num = X_cat\n",
    "            X_cat = None\n",
    "\n",
    "    D = Dataset(\n",
    "        X_num,\n",
    "        None,\n",
    "        y,\n",
    "        y_info={},\n",
    "        task_type=TaskType(df_info[\"task_type\"]),\n",
    "        n_classes=df_info[\"n_classes\"],\n",
    "    )\n",
    "\n",
    "    return transform_dataset(D, T, None, num_transform=num_transform), label_encoders, column_orders\n",
    "\n",
    "    \n",
    "\n",
    "def get_dataset(config_path =None, save_dir_tmp=None, train_name=\"train_with_id.csv\"):\n",
    "    configs, save_dir = load_configs(config_path)\n",
    "    # print(configs, save_dir)\n",
    "    #TBD: Customized Config res\n",
    "\n",
    "    # Display config\n",
    "    json_str = json.dumps(configs, indent=4)\n",
    "    # print(json_str)\n",
    "    print(configs[\"general\"][\"data_dir\"])\n",
    "\n",
    "    # Load  dataset\n",
    "\n",
    "    # In this step, we load the dataset according to the 'dataset_meta.json' file located in the data_dir.\n",
    "    tables, relation_order, dataset_meta = load_multi_table_customized(save_dir_tmp, meta_dir=\"/work4/xiaoyuwu/MIDSTModelsMIA/midst_models/single_table_TabDDPM/configs\", train_name=train_name)\n",
    "    print(\"\")\n",
    "\n",
    "    # Tables is a dictionary of the multi-table dataset\n",
    "    print(\n",
    "        \"{} We show the keys of the tables dictionary below {}\".format(\"=\" * 20, \"=\" * 20)\n",
    "    )\n",
    "    print(list(tables.keys()))\n",
    "\n",
    "    # Display important clustering parameters\n",
    "    params_clustering = configs[\"clustering\"]\n",
    "    print(\"{} We show the clustering parameters below {}\".format(\"=\" * 20, \"=\" * 20))\n",
    "    for key, val in params_clustering.items():\n",
    "        print(f\"{key}: {val}\")\n",
    "    print(\"\")\n",
    "\n",
    "    # Clustering on the multi-table dataset\n",
    "    tables, all_group_lengths_prob_dicts = clava_clustering(\n",
    "        tables, relation_order, save_dir, configs\n",
    "    )\n",
    "    train_loader_list = []\n",
    "    for parent, child in relation_order:\n",
    "        print(f\"Getting {parent} -> {child} model from scratch\")\n",
    "        df_with_cluster = tables[child][\"df\"]\n",
    "\n",
    "        id_cols = [col for col in df_with_cluster.columns if \"_id\" in col]\n",
    "        df_without_id = df_with_cluster.drop(columns=id_cols)\n",
    "        child_df_with_cluster, child_domain_dict, parent_name, child_name =  df_without_id, tables[child][\"domain\"], parent, child\n",
    "        if parent_name is None:\n",
    "            y_col = \"placeholder\"\n",
    "            child_df_with_cluster[\"placeholder\"] = list(range(len(child_df_with_cluster)))\n",
    "        else:\n",
    "            y_col = f\"{parent_name}_{child_name}_cluster\"\n",
    "        child_info = get_table_info(child_df_with_cluster, child_domain_dict, y_col)\n",
    "        child_model_params = get_model_params(\n",
    "            {\n",
    "                \"d_layers\": configs[\"diffusion\"][\"d_layers\"],\n",
    "                \"dropout\": configs[\"diffusion\"][\"dropout\"],\n",
    "            }\n",
    "        )\n",
    "        child_T_dict = get_T_dict()\n",
    "        file_path = os.path.join(save_dir_tmp, f\"{parent}_{child}_ckpt.pkl\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            model = CustomUnpickler(f).load()\n",
    "        diffusion = model['diffusion'].cuda()\n",
    "        num_transform = model['dataset'].num_transform\n",
    "        T = Transformations(**child_T_dict)\n",
    "        dataset, label_encoders, column_orders = make_dataset_from_df_with_loaded(\n",
    "            child_df_with_cluster,\n",
    "            T,\n",
    "            is_y_cond=child_model_params[\"is_y_cond\"],\n",
    "            ratios=[0.99, 0.005, 0.005],\n",
    "            df_info=child_info,\n",
    "            std=0,\n",
    "            label_encoders=model['label_encoders'],\n",
    "            num_transform=num_transform\n",
    "        )\n",
    "        # print(dataset.n_features)\n",
    "        dataset.X_num['test'] = dataset.X_num['train']\n",
    "        if dataset.X_cat is not None:\n",
    "            dataset.X_cat['test'] = dataset.X_cat['train']\n",
    "        dataset.y['test'] = dataset.y['train']\n",
    "        train_loader = prepare_fast_dataloader(\n",
    "            dataset, split=\"test\", batch_size=1, y_type=\"long\"\n",
    "        )\n",
    "        train_loader_list.append(train_loader)\n",
    "    return train_loader_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/work4/xiaoyuwu/MIDSTModelsMIA/starter_kits/tabddpm_white_box/train/tabddpm_1\n",
      "Table name: trans, Total dataframe shape: (200, 8), Numerical data shape: (200, 4), Categorical data shape: (200, 4)\n",
      "\n",
      "==================== We show the keys of the tables dictionary below ====================\n",
      "['trans']\n",
      "==================== We show the clustering parameters below ====================\n",
      "parent_scale: 1.0\n",
      "num_clusters: 50\n",
      "clustering_method: both\n",
      "\n",
      "Clustering checkpoint found, loading...\n",
      "Getting None -> trans model from scratch\n",
      "label_encoders loaded\n",
      "label_encoders loaded\n",
      "label_encoders loaded\n",
      "label_encoders loaded\n",
      "tensor([[-0.3191, -0.1172, -1.5632,  0.9795,  5.1993, -0.6125,  0.2794,  1.5003]],\n",
      "       device='cuda:0') {'y': tensor([0], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load config \n",
    "\n",
    "config_path = \"configs/trans_demo.json\"\n",
    "save_dir_tmp = '/work4/xiaoyuwu/MIDSTModelsMIA/starter_kits/tabddpm_white_box/train/tabddpm_1'\n",
    "train_loader_list = get_dataset(config_path, save_dir_tmp, train_name='challenge_with_id.csv')\n",
    "device = \"cuda\"\n",
    "\n",
    "for train_loader in train_loader_list:\n",
    "        x, out_dict = next(train_loader)\n",
    "        out_dict = {\"y\": out_dict}\n",
    "        x = x.to(device)\n",
    "        for k in out_dict:\n",
    "            out_dict[k] = out_dict[k].long().to(device)\n",
    "\n",
    "        print(x, out_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "midst-models-9WC0e6Lb-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
