{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5My1aIV2Tv"
   },
   "source": [
    "## Package Imports and Evironment Setup\n",
    "\n",
    "Ensure that you have installed the proper dependenices to run the notebook. The environment installation instructions are available [here](https://github.com/VectorInstitute/MIDSTModels/tree/main/starter_kits). Now that we have verfied we have the proper packages installed, lets import them and define global variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "MB3iIVMTFYyB"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Callable, Any\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from metrics import get_tpr_at_fpr\n",
    "TABDDPM_DATA_DIR = \"tabddpm_black_box\"\n",
    "TABSYN_DATA_DIR = \"tabsyn_black_box\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Material\n",
    "\n",
    "This part include all the source code used during preprocessing and dataset loading. But we need some manual modification to enable loading the trained normalizer and personalized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union, cast\n",
    "from pathlib import Path\n",
    "from dataclasses import astuple, dataclass, replace\n",
    "from midst_models.single_table_TabDDPM.lib import (\n",
    "    Transformations,\n",
    "    prepare_fast_dataloader,\n",
    "    round_columns,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.lib import Dataset, TaskType, transform_dataset\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_clustering_force_load,\n",
    "    clava_load_pretrained_customized,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_utils import load_multi_table_customized\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import CustomUnpickler\n",
    "\n",
    "\n",
    "CAT_MISSING_VALUE = \"__nan__\"\n",
    "CAT_RARE_VALUE = \"__rare__\"\n",
    "Normalization = Literal[\"standard\", \"quantile\", \"minmax\"]\n",
    "NumNanPolicy = Literal[\"drop-rows\", \"mean\"]\n",
    "CatNanPolicy = Literal[\"most_frequent\"]\n",
    "CatEncoding = Literal[\"one-hot\", \"counter\"]\n",
    "YPolicy = Literal[\"default\"]\n",
    "ArrayDict = Dict[str, np.ndarray]\n",
    "\n",
    "def raise_unknown(unknown_what: str, unknown_value: Any):\n",
    "    raise ValueError(f\"Unknown {unknown_what}: {unknown_value}\")\n",
    "\n",
    "\n",
    "def get_table_info(df, domain_dict, y_col):\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    for col in df.columns:\n",
    "        if col in domain_dict and col != y_col:\n",
    "            if domain_dict[col][\"type\"] == \"discrete\":\n",
    "                cat_cols.append(col)\n",
    "            else:\n",
    "                num_cols.append(col)\n",
    "\n",
    "    df_info = {}\n",
    "    df_info[\"cat_cols\"] = cat_cols\n",
    "    df_info[\"num_cols\"] = num_cols\n",
    "    df_info[\"y_col\"] = y_col\n",
    "    df_info[\"n_classes\"] = 0\n",
    "    df_info[\"task_type\"] = \"multiclass\"\n",
    "\n",
    "    return df_info\n",
    "\n",
    "def get_T_dict():\n",
    "    return {\n",
    "        \"seed\": 0,\n",
    "        \"normalization\": \"quantile\",\n",
    "        \"num_nan_policy\": None,\n",
    "        \"cat_nan_policy\": None,\n",
    "        \"cat_min_frequency\": None,\n",
    "        \"cat_encoding\": None,\n",
    "        \"y_policy\": \"default\",\n",
    "    }\n",
    "\n",
    "def get_model_params(rtdl_params=None):\n",
    "    return {\n",
    "        \"num_classes\": 0,\n",
    "        \"is_y_cond\": \"none\",\n",
    "        \"rtdl_params\": {\"d_layers\": [512, 1024, 1024, 1024, 1024, 512], \"dropout\": 0.0}\n",
    "        if rtdl_params is None\n",
    "        else rtdl_params,\n",
    "    }\n",
    "\n",
    "def build_target(\n",
    "    y: ArrayDict, policy: Optional[YPolicy], task_type: TaskType\n",
    ") -> Tuple[ArrayDict, Dict[str, Any]]:\n",
    "    info: Dict[str, Any] = {\"policy\": policy}\n",
    "    if policy is None:\n",
    "        pass\n",
    "    elif policy == \"default\":\n",
    "        if task_type == TaskType.REGRESSION:\n",
    "            mean, std = float(y[\"train\"].mean()), float(y[\"train\"].std())\n",
    "            y = {k: (v - mean) / std for k, v in y.items()}\n",
    "            info[\"mean\"] = mean\n",
    "            info[\"std\"] = std\n",
    "    else:\n",
    "        raise_unknown(\"policy\", policy)\n",
    "    return y, info\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Transformations:\n",
    "    seed: int = 0\n",
    "    normalization: Optional[Normalization] = None\n",
    "    num_nan_policy: Optional[NumNanPolicy] = None\n",
    "    cat_nan_policy: Optional[CatNanPolicy] = None\n",
    "    cat_min_frequency: Optional[float] = None\n",
    "    cat_encoding: Optional[CatEncoding] = None\n",
    "    y_policy: Optional[YPolicy] = \"default\"\n",
    "\n",
    "\n",
    "def transform_dataset(\n",
    "    dataset: Dataset,\n",
    "    transformations: Transformations,\n",
    "    cache_dir: Optional[Path],\n",
    "    transform_cols_num: int = 0,\n",
    "    normalizer=None,\n",
    "    cat_transform=None, \n",
    "    num_transform=None\n",
    ") -> Dataset:\n",
    "    # WARNING: the order of transformations matters. Moreover, the current\n",
    "    # implementation is not ideal in that sense.\n",
    "    if cache_dir is not None:\n",
    "        transformations_md5 = hashlib.md5(\n",
    "            str(transformations).encode(\"utf-8\")\n",
    "        ).hexdigest()\n",
    "        transformations_str = \"__\".join(map(str, astuple(transformations)))\n",
    "        cache_path = (\n",
    "            cache_dir / f\"cache__{transformations_str}__{transformations_md5}.pickle\"\n",
    "        )\n",
    "        if cache_path.exists():\n",
    "            cache_transformations, value = util.load_pickle(cache_path)\n",
    "            if transformations == cache_transformations:\n",
    "                # print(\n",
    "                #     f\"Using cached features: {cache_dir.name + '/' + cache_path.name}\"\n",
    "                # )\n",
    "                return value\n",
    "            else:\n",
    "                raise RuntimeError(f\"Hash collision for {cache_path}\")\n",
    "    else:\n",
    "        cache_path = None\n",
    "\n",
    "\n",
    "    cat_transform = None\n",
    "    X_num = dataset.X_num\n",
    "    X_num = {k: num_transform.transform(v) for k, v in X_num.items()}\n",
    "\n",
    "    if dataset.X_cat is None:\n",
    "        assert transformations.cat_nan_policy is None\n",
    "        assert transformations.cat_min_frequency is None\n",
    "        # assert transformations.cat_encoding is None\n",
    "        X_cat = None\n",
    "    else:\n",
    "        X_cat = cat_process_nans(dataset.X_cat, transformations.cat_nan_policy)\n",
    "        if transformations.cat_min_frequency is not None:\n",
    "            X_cat = cat_drop_rare(X_cat, transformations.cat_min_frequency)\n",
    "        \n",
    "        if cat_transform is None:\n",
    "            raise ValueError(\"See why no cat_tramsform\")\n",
    "        else:\n",
    "            X_cat = {k: cat_transform.transform(v).astype(\"float32\") for k, v in X_cat.items()} \n",
    "            X_num = (\n",
    "                X_cat\n",
    "                if X_num is None\n",
    "                else {x: np.hstack([X_num[x], X_cat[x]]) for x in X_num}\n",
    "            )\n",
    "            X_cat = None\n",
    "\n",
    "    y, y_info = build_target(dataset.y, transformations.y_policy, dataset.task_type)\n",
    "\n",
    "    dataset = replace(dataset, X_num=X_num, X_cat=X_cat, y=y, y_info=y_info)\n",
    "    dataset.num_transform = num_transform\n",
    "    dataset.cat_transform = cat_transform\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def make_dataset_from_df_with_loaded(df, T, is_y_cond, ratios=[0.7, 0.2, 0.1], df_info=None, std=0, label_encoders=None, num_transform=None):\n",
    "\n",
    "    cat_column_orders = []\n",
    "    num_column_orders = []\n",
    "    index_to_column = list(df.columns)\n",
    "    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n",
    "\n",
    "    if df_info[\"n_classes\"] > 0:\n",
    "        X_cat = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n",
    "        X_num = {} if df_info[\"num_cols\"] is not None else None\n",
    "        y = {}\n",
    "\n",
    "        cat_cols_with_y = []\n",
    "        if df_info[\"cat_cols\"] is not None:\n",
    "            cat_cols_with_y += df_info[\"cat_cols\"]\n",
    "        if is_y_cond == \"concat\":\n",
    "            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n",
    "\n",
    "        if len(cat_cols_with_y) > 0:\n",
    "            X_cat[\"train\"] = df[cat_cols_with_y].to_numpy(dtype=np.str_)\n",
    "\n",
    "        y[\"train\"] = df[df_info[\"y_col\"]].values.astype(np.float32)\n",
    "\n",
    "        if df_info[\"num_cols\"] is not None:\n",
    "            X_num[\"train\"] = df[df_info[\"num_cols\"]].values.astype(np.float32)\n",
    "\n",
    "        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n",
    "        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n",
    "\n",
    "    else:\n",
    "        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n",
    "        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n",
    "        y = {}\n",
    "\n",
    "        num_cols_with_y = []\n",
    "        if df_info[\"num_cols\"] is not None:\n",
    "            num_cols_with_y += df_info[\"num_cols\"]\n",
    "        if is_y_cond == \"concat\":\n",
    "            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n",
    "\n",
    "        if len(num_cols_with_y) > 0:\n",
    "            X_num[\"train\"] = df[num_cols_with_y].values.astype(np.float32)\n",
    "\n",
    "        y[\"train\"] = df[df_info[\"y_col\"]].values.astype(np.float32)\n",
    "\n",
    "        if df_info[\"cat_cols\"] is not None:\n",
    "            X_cat[\"train\"] = df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)\n",
    "\n",
    "        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n",
    "        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n",
    "\n",
    "    column_orders = num_column_orders + cat_column_orders\n",
    "    column_orders = [index_to_column[index] for index in column_orders]\n",
    "\n",
    "    if X_cat is not None and len(df_info[\"cat_cols\"]) > 0:\n",
    "        X_cat_all = X_cat[\"train\"]\n",
    "        X_cat_converted = []\n",
    "        for col_index in range(X_cat_all.shape[1]):\n",
    "            if label_encoders is None:\n",
    "                raise ValueError('Should be loaded: label_encoder')\n",
    "            else:\n",
    "                pass\n",
    "                # print('label_encoders loaded')\n",
    "\n",
    "            X_cat_converted.append(\n",
    "                label_encoders[col_index].transform(X_cat_all[:, col_index]).astype(float)\n",
    "            )\n",
    "            # print(X_cat_all[:, col_index].astype(float), X_cat_converted[-1])\n",
    "\n",
    "            # print(np.sum((X_cat_all[:, col_index].astype(float)- X_cat_converted[-1])**2))\n",
    "            \n",
    "            if std > 0:\n",
    "                # add noise\n",
    "                X_cat_converted[-1] += np.random.normal(\n",
    "                    0, std, X_cat_converted[-1].shape\n",
    "                )\n",
    "\n",
    "        X_cat_converted = np.vstack(X_cat_converted).T\n",
    "\n",
    "        train_num = X_cat[\"train\"].shape[0]\n",
    "\n",
    "        X_cat[\"train\"] = X_cat_converted[:train_num, :]\n",
    "\n",
    "        if len(X_num) > 0:\n",
    "            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n",
    "        else:\n",
    "            X_num = X_cat\n",
    "            X_cat = None\n",
    "\n",
    "    D = Dataset(\n",
    "        X_num,\n",
    "        None,\n",
    "        y,\n",
    "        y_info={},\n",
    "        task_type=TaskType(df_info[\"task_type\"]),\n",
    "        n_classes=df_info[\"n_classes\"],\n",
    "    )\n",
    "\n",
    "    return transform_dataset(D, T, None, num_transform=num_transform), label_encoders, column_orders\n",
    "\n",
    "\n",
    "def get_dataset(data_path, config_path =None, save_dir_tmp=None, train_name=\"train_with_id.csv\", phase=None):\n",
    "    configs, save_dir = load_configs(config_path)\n",
    "    tables, relation_order, dataset_meta = load_multi_table_customized(data_path, meta_dir=\"../midst_models/single_table_TabDDPM/configs\", train_name=train_name)\n",
    "    tables, all_group_lengths_prob_dicts = clava_clustering_force_load(\n",
    "        tables, relation_order, save_dir, configs\n",
    "    )\n",
    "    global batch_size\n",
    "    train_loader_list = []\n",
    "    for parent, child in relation_order:\n",
    "        # print(f\"Getting {parent} -> {child} model from scratch\")\n",
    "        df_with_cluster = tables[child][\"df\"]\n",
    "\n",
    "        id_cols = [col for col in df_with_cluster.columns if \"_id\" in col]\n",
    "        df_without_id = df_with_cluster.drop(columns=id_cols)\n",
    "\n",
    "        child_df_with_cluster, child_domain_dict, parent_name, child_name =  df_without_id, tables[child][\"domain\"], parent, child\n",
    "        if parent_name is None:\n",
    "            y_col = \"placeholder\"\n",
    "            child_df_with_cluster[\"placeholder\"] = list(range(len(child_df_with_cluster)))\n",
    "        else:\n",
    "            y_col = f\"{parent_name}_{child_name}_cluster\"\n",
    "        child_info = get_table_info(child_df_with_cluster, child_domain_dict, y_col)\n",
    "        child_model_params = get_model_params(\n",
    "            {\n",
    "                \"d_layers\": configs[\"diffusion\"][\"d_layers\"],\n",
    "                \"dropout\": configs[\"diffusion\"][\"dropout\"],\n",
    "            }\n",
    "        )\n",
    "        child_T_dict = get_T_dict()\n",
    "        file_path = os.path.join(save_dir_tmp, f\"{parent}_{child}_ckpt.pkl\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            model = CustomUnpickler(f).load()\n",
    "\n",
    "        diffusion = model['diffusion'].cuda()\n",
    "        num_transform = model['dataset'].num_transform\n",
    "        T = Transformations(**child_T_dict)\n",
    "\n",
    "        dataset, label_encoders, column_orders = make_dataset_from_df_with_loaded(\n",
    "            child_df_with_cluster,\n",
    "            T,\n",
    "            is_y_cond=child_model_params[\"is_y_cond\"],\n",
    "            ratios=[0.99, 0.005, 0.005],\n",
    "            df_info=child_info,\n",
    "            std=0,\n",
    "            label_encoders=model['label_encoders'],\n",
    "            num_transform=num_transform\n",
    "        )\n",
    "        # print(dataset.n_features)\n",
    "        dataset.X_num['test'] = dataset.X_num['train']\n",
    "\n",
    "        if dataset.X_cat is not None:\n",
    "            dataset.X_cat['test'] = dataset.X_cat['train']\n",
    "        dataset.y['test'] = dataset.y['train']\n",
    "        train_loader = prepare_fast_dataloader(\n",
    "            dataset, split=\"test\", batch_size=batch_size, y_type=\"long\"\n",
    "        )\n",
    "        train_loader_list.append([train_loader, dataset.X_num['test'].shape[0], dataset])\n",
    "    return train_loader_list\n",
    "\n",
    "from midst_models.single_table_TabDDPM.pipeline_utils import *\n",
    "def load_multi_table_customized(data_dir, meta_dir=None, train_name = \"train.csv\", verbose=True):\n",
    "    if meta_dir is None:\n",
    "        dataset_meta = json.load(open(os.path.join(data_dir, \"dataset_meta.json\"), \"r\"))\n",
    "    else:\n",
    "        dataset_meta =  json.load(open(os.path.join(meta_dir, \"dataset_meta.json\"), \"r\"))\n",
    "\n",
    "    relation_order = dataset_meta[\"relation_order\"]\n",
    "    relation_order_reversed = relation_order[::-1]\n",
    "\n",
    "    tables = {}\n",
    "\n",
    "    for table, meta in dataset_meta[\"tables\"].items():\n",
    "        if os.path.exists(os.path.join(data_dir, train_name)):\n",
    "            # print('exists')\n",
    "            train_df = pd.read_csv(os.path.join(data_dir, train_name))\n",
    "        else:\n",
    "            train_df = pd.read_csv(os.path.join(data_dir, f\"{table}.csv\"))\n",
    "\n",
    "        tables[table] = {\n",
    "            \"df\": train_df,\n",
    "            \"domain\": json.load(open(os.path.join(data_dir, f\"{table}_domain.json\"))),\n",
    "            \"children\": meta[\"children\"],\n",
    "            \"parents\": meta[\"parents\"],\n",
    "        }\n",
    "        tables[table][\"original_cols\"] = list(tables[table][\"df\"].columns)\n",
    "        tables[table][\"original_df\"] = tables[table][\"df\"].copy()\n",
    "        id_cols = [col for col in tables[table][\"df\"].columns if \"_id\" in col]\n",
    "        df_no_id = tables[table][\"df\"].drop(columns=id_cols)\n",
    "        info = get_info_from_domain(df_no_id, tables[table][\"domain\"])\n",
    "        data, info = pipeline_process_data(\n",
    "            name=table,\n",
    "            data_df=df_no_id,\n",
    "            info=info,\n",
    "            ratio=1,\n",
    "            save=False,\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        tables[table][\"info\"] = info\n",
    "\n",
    "    return tables, relation_order, dataset_meta\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This part we will try to load the pkl for training, validating and testing checkpoints to see what info provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Attack (Part1): Get the loss\n",
    "\n",
    "This part we basic copy paste the diffusion loss in path midst_models/single_table_TabDDPM/tab_ddpm/gaussian_multinomial_diffsuion.py. I just include necessary function and change all the self.xx to diffusion.xx to enable outside hijecking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_to_log_onehot(x, num_classes):\n",
    "    onehots = []\n",
    "    for i in range(len(num_classes)):\n",
    "        onehots.append(F.one_hot(x[:, i], num_classes[i]))\n",
    "\n",
    "    x_onehot = torch.cat(onehots, dim=1)\n",
    "    log_onehot = torch.log(x_onehot.float().clamp(min=1e-30))\n",
    "    return log_onehot\n",
    "\n",
    "noise=None\n",
    "t=None\n",
    "pt=None\n",
    "\n",
    "base_tabddpm_train_path = os.path.join(TABDDPM_DATA_DIR, \"train\")\n",
    "base_tabsyn_train_path = os.path.join(TABSYN_DATA_DIR, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing Loss Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract(a, t, x_shape):\n",
    "    b, *_ = t.shape\n",
    "    t = t.to(a.device)\n",
    "    out = a.gather(-1, t)\n",
    "    while len(out.shape) < len(x_shape):\n",
    "        out = out[..., None]\n",
    "    return out.expand(x_shape)\n",
    "\n",
    "\n",
    "def mixed_loss(diffusion, x, out_dict, noise=None, t=None, pt=None, return_random=False, no_mean=False):\n",
    "    global guidance_scale\n",
    "    device = x.device\n",
    "\n",
    "    x_num = x[:, : diffusion.num_numerical_features]\n",
    "    x_cat = x[:, diffusion.num_numerical_features :]\n",
    "\n",
    "    global input_noise\n",
    "    global parallel_batch\n",
    "    if x_num.shape[1] > 0:\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_num)\n",
    "    noise = input_noise\n",
    "    noise_tensor = torch.tensor(noise, device='cuda', dtype=torch.float)\n",
    "\n",
    "    batch_noise = noise_tensor.repeat(x_num.shape[0], 1) \n",
    "\n",
    "    x_num = x_num.repeat_interleave(parallel_batch, dim=0)  # shape: [100000, 8]\n",
    "    x_cat = x_cat.repeat_interleave(parallel_batch, dim=0)  # shape: [100000, 8]\n",
    "\n",
    "    b = x_num.shape[0]\n",
    "\n",
    "    log_x_cat_t = x_cat\n",
    "\n",
    "    if t is None:\n",
    "        t, pt = diffusion.sample_time(b, device, \"uniform\")\n",
    "\n",
    "    if return_random:\n",
    "        return noise, t, pt \n",
    "   \n",
    "\n",
    "    x_input_noise = diffusion.gaussian_q_sample(x_num, t, noise=batch_noise)\n",
    "\n",
    "    if not return_random:\n",
    "        current_t = t\n",
    "        current_guidance_scale = 1\n",
    "        predicted_noise = diffusion._denoise_fn(x_input_noise, current_t, **out_dict)\n",
    "        scaled_xt_noise = extract(diffusion.sqrt_alphas_cumprod, current_t, x_input_noise.shape) * x_input_noise\n",
    "        predicted_noise = predicted_noise + (current_guidance_scale-1) * (predicted_noise-scaled_xt_noise)\n",
    "        global saved_noise\n",
    "        global noise_batch_id\n",
    "        current_loss = diffusion._gaussian_loss(predicted_noise, batch_noise, batch_noise, current_t, batch_noise)\n",
    "        transformed_current_loss = current_loss.reshape(-1, parallel_batch)\n",
    "\n",
    "    return transformed_current_loss*0, transformed_current_loss\n",
    "\n",
    "\n",
    "\n",
    "def mixed_loss_final(diffusion, x, out_dict, noise=None, t=None, pt=None, return_random=False, no_mean=False):\n",
    "    global guidance_scale\n",
    "    global rv\n",
    "    b = x.shape[0]\n",
    "    device = x.device\n",
    "    if t is None:\n",
    "        t, pt = diffusion.sample_time(b, device, \"uniform\")\n",
    "\n",
    "    x_num = x[:, : diffusion.num_numerical_features]\n",
    "    x_cat = x[:, diffusion.num_numerical_features :]\n",
    "\n",
    "    log_x_cat_t = x_cat\n",
    "    if x_num.shape[1] > 0:\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_num)\n",
    "    if return_random:\n",
    "        return noise, t, pt \n",
    "    \n",
    "    noise = noise.float().to(device='cuda')\n",
    "\n",
    "    x_num_t = diffusion.gaussian_q_sample(x_num, t, noise=noise)\n",
    "    \n",
    "    loss_multi = torch.zeros((1,)).float()\n",
    "    loss_gauss = torch.zeros((1,)).float()\n",
    "\n",
    "    x_in = torch.cat([x_num_t, log_x_cat_t], dim=1)\n",
    "    \n",
    "    model_out = diffusion._denoise_fn(x_in, t, **out_dict)\n",
    "\n",
    "    model_out_num = model_out[:, : diffusion.num_numerical_features]\n",
    "\n",
    "    scaled_xt = extract(diffusion.sqrt_alphas_cumprod, t, x_in.shape) * x_in\n",
    "    # scaled_xt = extract(diffusion.sqrt_one_minus_alphas_cumprod, t, x_in.shape) * x_in\n",
    "    scaled_xt_num = scaled_xt[:, : diffusion.num_numerical_features]\n",
    "    # scaled_xt = extract(diffusion.sqrt_alphas_cumprod, t, model_out_num.shape) * model_out_num\n",
    "    \n",
    "    model_out_num = model_out_num + (guidance_scale-1) * (model_out_num-scaled_xt_num)\n",
    "    \n",
    "    if x_num.shape[1] > 0:\n",
    "        loss_gauss = diffusion._gaussian_loss(model_out_num, x_num, x_num_t, t, noise)\n",
    "    \n",
    "    if x_cat.shape[1] > 0:\n",
    "        raise ValueError()\n",
    "    if no_mean:\n",
    "        return loss_multi, loss_gauss\n",
    "    return loss_multi.mean(), loss_gauss.mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train our own Regression Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)  \n",
    "    def forward(self, x):\n",
    "        residual = torch.tanh(self.fc1(x))  \n",
    "        residual = torch.tanh(self.fc2(residual)) \n",
    "        output = torch.sigmoid(self.fc3(residual))\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss_fn(model, X, y, fpr_target=0.5):\n",
    "    confidences = model(X)\n",
    "    X = X.float()\n",
    "    y = y.float()\n",
    "    mse_loss = nn.BCELoss()(confidences, y.unsqueeze(1))\n",
    "\n",
    "    return mse_loss\n",
    "\n",
    "\n",
    "# def fitmodel(regression_model, X, y, fpr_target=0.5, num_epochs=100, learning_rate=1e-3):\n",
    "def fitmodel(regression_model, X_train, X_label, X_test, X_label2, fpr_target=0.5, num_epochs=1000, learning_rate=1e-4):\n",
    "    global test_set_ratio\n",
    "    global USE_BEST_CHECKPOINT\n",
    "    \n",
    "    optimizer = optim.Adam(regression_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # -----------------------------new one-------------------\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).cuda()\n",
    "    y_train = torch.tensor(X_label, dtype=torch.float32).cuda()\n",
    "    indices = torch.randperm(X_train.size(0)).cuda()\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).cuda()\n",
    "    y_test = torch.tensor(X_label2, dtype=torch.float32).cuda()\n",
    "\n",
    "\n",
    "    #  --------------------------------------------------- \n",
    "\n",
    "    X_train.requires_grad = True\n",
    "    y_train.requires_grad = True\n",
    "    train_loss_res = []\n",
    "    test_loss_res = []\n",
    "    train_tpr_res = []\n",
    "    test_tpr_res = []\n",
    "    epoch_plot = []\n",
    "    regression_model.train()\n",
    "    best_tpr = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = custom_loss_fn(regression_model, X_train, y_train, fpr_target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        with torch.no_grad():\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                train_loss_res.append(loss.item())\n",
    "                epoch_plot.append(epoch)\n",
    "                tpr_at_fpr = get_tpr_at_fpr(y_train.detach().cpu().numpy(), regression_model(X_train).detach().cpu().numpy())\n",
    "                train_tpr_res.append(tpr_at_fpr)\n",
    "                if test_set_ratio > 0:\n",
    "                    test_loss = custom_loss_fn(regression_model, X_test, y_test, fpr_target)\n",
    "                    test_tpr_at_fpr = get_tpr_at_fpr(y_test.detach().cpu().numpy(), regression_model(X_test).detach().cpu().numpy())\n",
    "                    test_loss_res.append(test_loss.item())\n",
    "                    test_tpr_res.append(test_tpr_at_fpr)\n",
    "                    if test_tpr_at_fpr> best_tpr:\n",
    "                        best_tpr = test_tpr_at_fpr\n",
    "                        torch.save(regression_model.state_dict(), 'best_model.pt')\n",
    "  \n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()} Test Loss :{test_loss.item()} Train TPR: {tpr_at_fpr} Test TPR: {test_tpr_at_fpr}\")\n",
    "                else:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()} Train TPR: {tpr_at_fpr}\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epoch_plot, train_loss_res, label='Train Loss', color='blue')\n",
    "    if test_set_ratio > 0:\n",
    "        plt.plot(epoch_plot, test_loss_res, label='Test Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epoch_plot, train_tpr_res, label='Train TPR', color='green')\n",
    "    if test_set_ratio > 0:\n",
    "        plt.plot(epoch_plot, test_tpr_res, label='Test TPR', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('Train and Test TPR')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    if USE_BEST_CHECKPOINT:\n",
    "        regression_model.load_state_dict(torch.load('best_model.pt'))\n",
    "    test_loss = custom_loss_fn(regression_model, X_test, y_test, fpr_target)\n",
    "    test_tpr_at_fpr =  get_tpr_at_fpr(y_test.detach().cpu().numpy(), regression_model(X_test).detach().cpu().numpy())\n",
    "    print(f'final best loss is {test_loss} best tpr is {test_tpr_at_fpr}')\n",
    "    return regression_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Attack (Part2): \n",
    "\n",
    "This part we are trying to get a score based on any input models. The basic logit is to  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import clava_clustering\n",
    "import json\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import child_training\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_normalizer(\n",
    "    X_train,\n",
    "    normalization,\n",
    "    seed,\n",
    "    return_normalizer: bool = True,\n",
    ") :\n",
    "    # print(max(min(X_train.shape[0] // 30, 1000), 10))    \n",
    "    if normalization == \"standard\":\n",
    "        normalizer = sklearn.preprocessing.StandardScaler()\n",
    "    elif normalization == \"minmax\":\n",
    "        normalizer = sklearn.preprocessing.MinMaxScaler()\n",
    "    elif normalization == \"quantile\":\n",
    "        normalizer = sklearn.preprocessing.QuantileTransformer(\n",
    "            output_distribution=\"normal\",\n",
    "            n_quantiles=max(min(X_train.shape[0] // 30, 1000), 10),\n",
    "            subsample=int(1e9),\n",
    "            random_state=seed,\n",
    "        )\n",
    "\n",
    "    normalizer.fit(X_train)\n",
    "   \n",
    "    return normalizer\n",
    "\n",
    "\n",
    "# The function is directly copied from MIDSTModelsMIA/midst_models/single_table_TabDDPM/pipeline_modules.py l68\n",
    "def get_T_dict():\n",
    "    return {\n",
    "        \"seed\": 0,\n",
    "        \"normalization\": \"quantile\",\n",
    "        \"num_nan_policy\": None,\n",
    "        \"cat_nan_policy\": None,\n",
    "        \"cat_min_frequency\": None,\n",
    "        \"cat_encoding\": None,\n",
    "        \"y_policy\": \"default\",\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# This function is part of my implementation and may contain some bugs.\n",
    "def get_score(data_path, save_dir, config_path=None, type='tabddpm', phase=None):\n",
    "    \"\"\"\n",
    "    Calculate the score for the given input.\n",
    "\n",
    "    Parameters:\n",
    "    save_dir (str): Path to the saved model.\n",
    "    x (Any): The input challenge point to be evaluated.\n",
    "    type (str): Model type to use for scoring. Default is 'tabddpm'.\n",
    "    num_transform (Any, optional): A global transformation used for preprocessing the dataset.\n",
    "\n",
    "    Returns:\n",
    "    Any: The calculated score.\n",
    "    \"\"\"\n",
    "    global challenge_name\n",
    "    if type == 'tabddpm':\n",
    "        relation_order=[(\"None\", \"trans\")]\n",
    "    elif type == 'tabsyn':\n",
    "        raise ValueError(\"Haven't done it yet!\")\n",
    "    train_loader_list = get_dataset(data_path, config_path, save_dir, train_name=challenge_name, phase=phase)\n",
    "    \n",
    "\n",
    "    # for tabddpm, relation order only contains like None_trans\n",
    "    # This code is not so good in the sense that it need to load the model once for each datapoint. It could be faster to move the loading process outside this function\n",
    "    loader_count = 0\n",
    "    global noise_batch_id\n",
    "    global parallel_batch\n",
    "\n",
    "    for parent, child in relation_order:\n",
    "        assert os.path.exists(\n",
    "            os.path.join(save_dir, f\"{parent}_{child}_ckpt.pkl\")\n",
    "        )\n",
    "        train_loader, iter_max, challenge_dataset = train_loader_list[loader_count]\n",
    "\n",
    "        filepath = os.path.join(save_dir, f\"{parent}_{child}_ckpt.pkl\")\n",
    "        # print(f\"{parent} -> {child} checkpoint found, loading...\")\n",
    "        \n",
    "        \n",
    "        with open(filepath, \"rb\") as f:\n",
    "            model = CustomUnpickler(f).load()\n",
    "        diffusion = model['diffusion'].cuda()\n",
    "\n",
    "        device = 'cuda'\n",
    "        iter_id = 0\n",
    "        global batch_size\n",
    "\n",
    "        iter_max = iter_max//batch_size\n",
    "        # return_res = torch.zeros([batch_size*parallel_batch, 1])\n",
    "        eturn_res = torch.zeros([batch_size, parallel_batch])\n",
    "        assert iter_max == 1\n",
    "        iter_id = 0\n",
    "        while iter_id < iter_max:\n",
    "            \n",
    "            x, out_dict = next(train_loader)\n",
    "            out_dict = {\"y\": out_dict}\n",
    "            x = x.to(device)\n",
    "            for k in out_dict:\n",
    "                out_dict[k] = out_dict[k].long().to(device)\n",
    "            # print(x, out_dict, iter_id)\n",
    "            # This part we want to fix the random variables noise, t, pt. So they are dealing as gloabl variable\n",
    "            global noise\n",
    "            global t\n",
    "            global t_value\n",
    "            global pt\n",
    "\n",
    "            # This loss_dataset is an indicator to show statistic information about the training data (And we want to show it only once)\n",
    "            global loss_dataset\n",
    "            with torch.no_grad():\n",
    "         \n",
    "                noise, _, pt =  mixed_loss (diffusion, x, out_dict, return_random=True)\n",
    "                t = _ * 0 + t_value\n",
    "                _, loss = mixed_loss (diffusion, x, out_dict, t=t, noise=noise, pt=pt, no_mean=True)\n",
    "\n",
    "            return_res = loss\n",
    "            iter_id += 1\n",
    "    return return_res\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data to get evaluation\n",
    "\n",
    "Now we can just go through all the dataset and get the prediction based on the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "# phases = [\"src_train\", \"train\"]\n",
    "# phases = [\"train\", \"dev\", \"final\"]\n",
    "# phases = [\"train\"]\n",
    "import inspect\n",
    "import copy\n",
    "# It is used to remove uncessary args within the dict during processing normalizer\n",
    "def filter_extra_args(func, arg_dict):\n",
    "    valid_args = inspect.signature(func).parameters\n",
    "    return {key: value for key, value in arg_dict.items() if key in valid_args}\n",
    "\n",
    "# The function is used to get loss as input and derive some kind of prediction confidence\n",
    "# It is not a so-good function. I just implement a pretty simple one.\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def process_loss(predictions):\n",
    "    \"\"\"\n",
    "    Process loss values for membership determination:\n",
    "    1. Filter outliers: Set values greater than twice the median loss to 0. (Hihger loss should have lower confidence to be membership)\n",
    "    2. Apply reverse normalization to the remaining values, where larger loss values correspond to smaller normalized values.\n",
    "\n",
    "    Parameters:\n",
    "    predictions (torch.Tensor): Input tensor containing loss values.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Processed tensor with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    median_val = predictions.median()\n",
    "    # print(f\"Median: {median_val.item()}\")\n",
    "    # median_val = predictions.mean()\n",
    "    # print(f\"Mean: {median_val.item()}\")\n",
    "\n",
    "    filtered_predictions = torch.where(\n",
    "        predictions > 2 * median_val, \n",
    "        torch.tensor(0.0, dtype=predictions.dtype, device=predictions.device), \n",
    "        predictions\n",
    "    )\n",
    "\n",
    "    non_zero_values = filtered_predictions[filtered_predictions > 0]\n",
    "    if len(non_zero_values) == 0:\n",
    "        raise ValueError(\"All values are outliers after filtering!\")\n",
    "    min_val = non_zero_values.min()\n",
    "    max_val = non_zero_values.max()\n",
    "    # print(f\"Min: {min_val.item()}, Max: {max_val.item()}\")\n",
    "    normalized = (max_val - filtered_predictions) / (max_val - min_val)\n",
    "    normalized = torch.where(filtered_predictions > 0, normalized, torch.tensor(0.0, dtype=predictions.dtype, device=predictions.device))\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "def process_loss2(predictions):\n",
    "    \"\"\"\n",
    "    Process loss values for membership determination:\n",
    "    1. Rank the loss values and assign ith with score i / number_of_samples.\n",
    "    \n",
    "    Parameters:\n",
    "    predictions (torch.Tensor): Input tensor containing loss values.\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Processed tensor with values in the range [0, 1].\n",
    "    \"\"\"\n",
    "    # Rank the loss values (sorted in ascending order)\n",
    "    predictions = predictions.squeeze()\n",
    "    sorted_indices = torch.argsort(predictions, descending=True)\n",
    "    sorted_values = predictions[sorted_indices]\n",
    "    \n",
    "    # Total number of values\n",
    "    valid_values_count = len(sorted_values)\n",
    "    \n",
    "    # Assign ranks and scale them to [0, 1], where rank 1 corresponds to 1/n, rank 2 to 2/n, ...\n",
    "    ranks = torch.arange(1, valid_values_count + 1, dtype=torch.float32, device=predictions.device)\n",
    "    scores = ranks / valid_values_count\n",
    "\n",
    "    # Reassign scores back to the original predictions\n",
    "    processed_predictions = torch.zeros_like(predictions)\n",
    "    processed_predictions[sorted_indices] = scores\n",
    "    \n",
    "    return processed_predictions.unsqueeze(1)\n",
    "\n",
    "NEW_MODEL = \"workspace/train_1/models\"\n",
    "\n",
    "# @torch.no_grad()\n",
    "def main_function_process():\n",
    "    global regression_model\n",
    "    global noise_num_sample\n",
    "    global t_value\n",
    "    global addt_value\n",
    "    global sample_num\n",
    "    global challenge_label_name\n",
    "    noise_count = 0\n",
    "    train_noise_count = 0\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    for base_dir, model_type in zip([TABDDPM_DATA_DIR], ['tabddpm']):\n",
    "        # if model_type == 'tabddpm':\n",
    "        #     config_path = \"../midst_models/single_table_TabDDPM/configs/trans_demo.json\"\n",
    "        \n",
    "        # 6000, noise_num (200) * t_num (5)  \n",
    "        for phase in phases:\n",
    "            if phase == 'src_train':\n",
    "                root = os.path.join(base_dir, \"train\")\n",
    "            else:\n",
    "                root = os.path.join(base_dir, phase)\n",
    "\n",
    "            index = 0\n",
    "            global X_TRAIN\n",
    "            global X_LABEL\n",
    "            global X_TEST\n",
    "            global X_LABEL2\n",
    "            global challenge_name\n",
    "            global batch_size\n",
    "            global train_indices\n",
    "            for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):                      \n",
    "                global loss_dataset\n",
    "                global t\n",
    "                global pt\n",
    "                # Reset global varibale for each model\n",
    "                loss_dataset = False\n",
    "                t=None\n",
    "                pt=None\n",
    "                path = os.path.join(root, model_folder)\n",
    "                config_path = os.path.join(path, \"trans_demo.json\")\n",
    "                model_path = os.path.join(path, NEW_MODEL)\n",
    "                global df_train_merge\n",
    "                global df_test_merge\n",
    "                global DATA_PER_MODEL\n",
    "                global TEST_DATA_MODEL\n",
    "\n",
    "                #  src_train part, train the model?\n",
    "                if phase == 'src_train':\n",
    "                    # for train models, collect data to \"data.csv\"\n",
    "\n",
    "                    if index in train_indices:\n",
    "                        # CREATE A NEW DATA FILE!!!!\n",
    "\n",
    "                        df_train = pd.read_csv(os.path.join(path, \"train_with_id.csv\"))\n",
    "                        ## get data not chosen before and not in training set\n",
    "                        df_exclusive = df_train_merge[~df_train_merge.set_index([\"trans_id\", \"balance\"]).index.isin(\n",
    "                            df_train.set_index([\"trans_id\", \"balance\"]).index\n",
    "                        )]\n",
    "\n",
    "                        #######\n",
    "                        data_exclusive = df_exclusive.sample(DATA_PER_MODEL)\n",
    "\n",
    "                        #######\n",
    "                        data_from_train = df_train.sample(DATA_PER_MODEL)\n",
    "\n",
    "                        # for data.csv\n",
    "                        df_data = pd.concat([data_exclusive, data_from_train], ignore_index=True)\n",
    "                        df_data.to_csv(os.path.join(path, \"data.csv\"), index=False)\n",
    "\n",
    "                        # \n",
    "                        df_train_merge = df_train_merge[~df_train_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_data.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "                        # df_train_merge = df_train_merge[~df_train_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                        #     (data_from_train.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "                    else:\n",
    "                        df_test = pd.read_csv(os.path.join(path, \"train_with_id.csv\"))\n",
    "                        df_exclusive = df_test_merge[~df_test_merge.set_index([\"trans_id\", \"balance\"]).index.isin(\n",
    "                            df_test.set_index([\"trans_id\", \"balance\"]).index\n",
    "                        )]\n",
    "\n",
    "                        #######\n",
    "                        data_test_exclusive = df_exclusive.sample(TEST_DATA_MODEL)\n",
    "\n",
    "                        #######\n",
    "                        data_from_test = df_test.sample(TEST_DATA_MODEL)\n",
    "\n",
    "                        # for data.csv\n",
    "                        df_test_data = pd.concat([data_test_exclusive, data_from_test], ignore_index=True)\n",
    "                        df_test_data.to_csv(os.path.join(path, \"data.csv\"), index=False)\n",
    "\n",
    "                        # \n",
    "                        df_test_merge = df_test_merge[~df_test_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_test_data.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "                        # df_test_merge = df_test_merge[~df_test_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                        #     (data_from_test.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "\n",
    "\n",
    "\n",
    "                    t_value_count = 0\n",
    "                    for t_value in t_value_list:\n",
    "                        for addt_value in addt_value_list:\n",
    "                            if index in train_indices:    \n",
    "                                challenge_name = \"data.csv\"\n",
    "                                batch_size = DATA_PER_MODEL * 2\n",
    "                                predictions = get_score(path, model_path, config_path, model_type, phase=\"train\") \n",
    "                                # print(\"index = \", index, predictions.shape)\n",
    "\n",
    "                                X_TRAIN[\n",
    "                                    DATA_PER_MODEL * 2 * train_count : DATA_PER_MODEL * 2 * (train_count + 1),\n",
    "                                    t_value_count * noise_num_sample : (t_value_count + 1) * noise_num_sample\n",
    "                                ] = (\n",
    "                                    predictions.detach().squeeze().cpu().numpy()\n",
    "                                )\n",
    "\n",
    "                                X_LABEL[DATA_PER_MODEL*2*train_count : DATA_PER_MODEL*2*(train_count+1)] = np.concatenate([np.zeros(DATA_PER_MODEL), np.ones(DATA_PER_MODEL)])\n",
    "\n",
    "                                t_value_count += 1\n",
    "                            \n",
    "                            else:\n",
    "                                # test model \n",
    "                                # challenge_name = 'challenge_with_id.csv'\n",
    "                                # batch_size = 200\n",
    "                                challenge_name = \"data.csv\"\n",
    "                                batch_size = TEST_DATA_MODEL * 2\n",
    "                                predictions = get_score(path, model_path, config_path, model_type, phase=\"train\")\n",
    "                                # print(\"index = \", index, predictions.shape)\n",
    "                                \n",
    "                                # get 200*noise_num_sample noise, then store (200, noise_num), at modeli*200 position, with t_value_count*noise_num each data\n",
    "                                # X_TEST[200*test_count:200*test_count+200, t_value_count*noise_num_sample:(t_value_count+1)*noise_num_sample] = predictions.detach().squeeze().cpu().numpy()\n",
    "                                # solution = np.loadtxt(os.path.join(path, challenge_label_name), skiprows=1)\n",
    "                                X_TEST[\n",
    "                                    TEST_DATA_MODEL * 2 * test_count : TEST_DATA_MODEL * 2 * (test_count + 1),\n",
    "                                    t_value_count * noise_num_sample : (t_value_count + 1) * noise_num_sample\n",
    "                                ] = (\n",
    "                                    predictions.detach().squeeze().cpu().numpy()\n",
    "                                )\n",
    "\n",
    "                                X_LABEL2[TEST_DATA_MODEL*2*test_count : TEST_DATA_MODEL*2*(test_count+1)] = np.concatenate([np.zeros(TEST_DATA_MODEL), np.ones(TEST_DATA_MODEL)])\n",
    "\n",
    "\n",
    "                                # store solution for the 200 data\n",
    "                                # X_LABEL2[200*test_count:200*test_count+200] = solution\n",
    "                                t_value_count += 1\n",
    "                        \n",
    "                        # predictions = get_score(path, model_path, config_path, model_type, phase=\"train\")\n",
    "                        \n",
    "                        # train_loss_list[200*train_noise_count:200*train_noise_count+200, t_value_count*noise_num_sample:(t_value_count+1)*noise_num_sample] = predictions.detach().squeeze().cpu().numpy()\n",
    "                        # solution = np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1)\n",
    "\n",
    "                        # train_loss_label[200*train_noise_count:200*train_noise_count+200] = solution\n",
    "                        # t_value_count += 1\n",
    "                    if index in train_indices:\n",
    "                        train_count += 1\n",
    "                        print(\"train\", train_count, index)\n",
    "                    else:\n",
    "                        test_count+=1\n",
    "                        print(\"test\", test_count, index)\n",
    "                    index += 1\n",
    "                else:\n",
    "                    batch_size = 200\n",
    "                    challenge_name = \"challenge_with_id.csv\"\n",
    "                    t_value_count = 0\n",
    "                    current_input = []\n",
    "                    for t_value in t_value_list:\n",
    "                        for addt_value in addt_value_list:\n",
    "                            predictions = get_score(path, model_path, config_path, model_type, phase=phase) \n",
    "                            t_value_count += 1\n",
    "                            current_input = current_input + [predictions]\n",
    "                    predictions = torch.cat(current_input, dim=-1)\n",
    "\n",
    "                    predictions = regression_model(predictions).detach().cpu().numpy()\n",
    "                    # to [0, 1]\n",
    "                    min_output, max_output = np.min(predictions), np.max(predictions)\n",
    "                    predictions = (predictions - min_output) / (max_output - min_output)\n",
    "\n",
    "                    predictions = torch.tensor(predictions)\n",
    "\n",
    "\n",
    "                    assert torch.all((0 <= predictions) & (predictions <= 1))\n",
    "\n",
    "                    with open(os.path.join(path, \"prediction.csv\"), mode=\"w\", newline=\"\") as file:\n",
    "                        writer = csv.writer(file)\n",
    "            \n",
    "                        # Write each value in a separate row\n",
    "                        for value in list(predictions.numpy().squeeze()):\n",
    "                            writer.writerow([value])\n",
    "\n",
    "                    noise_count += 1\n",
    "            if phase == 'src_train':\n",
    "                # fitmodel(regression_model, train_loss_list.reshape(-1, 1), train_loss_label.reshape(-1, 1))\n",
    "                global NUM_Epochs\n",
    "                fitmodel(regression_model, X_TRAIN, X_LABEL, X_TEST, X_LABEL2, num_epochs=NUM_Epochs)\n",
    "    tpr_at_fpr_list = []\n",
    "    tpr_at_fpr2_list = []\n",
    "\n",
    "    for base_dir in [TABDDPM_DATA_DIR]:\n",
    "        predictions = []\n",
    "        predictions2 = []\n",
    "        solutions  = []\n",
    "        root = os.path.join(base_dir, \"train\")\n",
    "        global global_noise_count\n",
    "        global saved_tpr\n",
    "   \n",
    "        for i, model_folder in enumerate(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1]))):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "            # predictions2.append(np.loadtxt(os.path.join(path, \"prediction2.csv\")))\n",
    "            solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "        predictions = np.concatenate(predictions)\n",
    "        # predictions2 = np.concatenate(predictions2)\n",
    "        solutions = np.concatenate(solutions)\n",
    "        \n",
    "        tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "        tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "        # tpr_at_fpr2 = get_tpr_at_fpr(solutions, predictions2)\n",
    "        # tpr_at_fpr2_list.append(tpr_at_fpr2)\n",
    "    final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "    final_tpr_at_fpr2 = 0\n",
    "    return final_tpr_at_fpr, final_tpr_at_fpr2\n",
    "# t_value_list = [10,15,20,25,30]\n",
    "# t_value_list = [5, 10, 15, 20, 25, 30]\n",
    "# t_value_list = [10,15,20, 30,50, 100]\n",
    "\n",
    "# t_value_list = [5,10,15,20]\n",
    "# addt_value_list = [0,2,5,10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## simple run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of merge train data 332075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=99'>100</a>\u001b[0m \u001b[39mfor\u001b[39;00m noise_batch_id \u001b[39min\u001b[39;00m tqdm(\u001b[39mrange\u001b[39m(noise_batch_num)):\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=100'>101</a>\u001b[0m     input_noise \u001b[39m=\u001b[39m input_noise_list[noise_batch_id \u001b[39m*\u001b[39m parallel_batch: (noise_batch_id\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m parallel_batch]\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=102'>103</a>\u001b[0m     final_tpr_at_fpr, final_tpr_at_fpr2 \u001b[39m=\u001b[39m main_function_process()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=103'>104</a>\u001b[0m     plot_res\u001b[39m.\u001b[39mappend(final_tpr_at_fpr)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=104'>105</a>\u001b[0m     \u001b[39mprint\u001b[39m(final_tpr_at_fpr, final_tpr_at_fpr2)\n",
      "\u001b[1;32m/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=190'>191</a>\u001b[0m challenge_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdata.csv\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=191'>192</a>\u001b[0m batch_size \u001b[39m=\u001b[39m DATA_PER_MODEL \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=192'>193</a>\u001b[0m predictions \u001b[39m=\u001b[39m get_score(path, model_path, config_path, model_type, phase\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m\"\u001b[39;49m) \n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=193'>194</a>\u001b[0m \u001b[39m# print(\"index = \", index, predictions.shape)\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=195'>196</a>\u001b[0m X_TRAIN[\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=196'>197</a>\u001b[0m     DATA_PER_MODEL \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m train_count : DATA_PER_MODEL \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m*\u001b[39m (train_count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=197'>198</a>\u001b[0m     t_value_count \u001b[39m*\u001b[39m noise_num_sample : (t_value_count \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m noise_num_sample\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=198'>199</a>\u001b[0m ] \u001b[39m=\u001b[39m (\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=199'>200</a>\u001b[0m     predictions\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=200'>201</a>\u001b[0m )\n",
      "\u001b[1;32m/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=132'>133</a>\u001b[0m     noise, _, pt \u001b[39m=\u001b[39m  mixed_loss (diffusion, x, out_dict, return_random\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=133'>134</a>\u001b[0m     t \u001b[39m=\u001b[39m _ \u001b[39m*\u001b[39m \u001b[39m0\u001b[39m \u001b[39m+\u001b[39m t_value\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=134'>135</a>\u001b[0m     _, loss \u001b[39m=\u001b[39m mixed_loss (diffusion, x, out_dict, t\u001b[39m=\u001b[39;49mt, noise\u001b[39m=\u001b[39;49mnoise, pt\u001b[39m=\u001b[39;49mpt, no_mean\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=136'>137</a>\u001b[0m return_res \u001b[39m=\u001b[39m loss\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=137'>138</a>\u001b[0m iter_id \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[1;32m/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb Cell 17\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=43'>44</a>\u001b[0m current_t \u001b[39m=\u001b[39m t\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=44'>45</a>\u001b[0m current_guidance_scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=45'>46</a>\u001b[0m predicted_noise \u001b[39m=\u001b[39m diffusion\u001b[39m.\u001b[39;49m_denoise_fn(x_input_noise, current_t, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mout_dict)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m scaled_xt_noise \u001b[39m=\u001b[39m extract(diffusion\u001b[39m.\u001b[39msqrt_alphas_cumprod, current_t, x_input_noise\u001b[39m.\u001b[39mshape) \u001b[39m*\u001b[39m x_input_noise\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bcmu/work4/xiaoyuwu/Tartan_Federer_MIDST/MIA/MLP_blackbox_more_data_FINAL.ipynb#X26sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m predicted_noise \u001b[39m=\u001b[39m predicted_noise \u001b[39m+\u001b[39m (current_guidance_scale\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (predicted_noise\u001b[39m-\u001b[39mscaled_xt_noise)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/midst-models-9WC0e6Lb-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/work4/xiaoyuwu/MIDSTModelsMIA/midst_models/single_table_TabDDPM/tab_ddpm/modules.py:461\u001b[0m, in \u001b[0;36mMLPDiffusion.forward\u001b[0;34m(self, x, timesteps, y)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, timesteps, y\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 461\u001b[0m     emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime_embed(timestep_embedding(timesteps, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdim_t))\n\u001b[1;32m    462\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mis_y_cond \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/midst-models-9WC0e6Lb-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/midst-models-9WC0e6Lb-py3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:204\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    203\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 204\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    205\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/midst-models-9WC0e6Lb-py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/midst-models-9WC0e6Lb-py3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "phases = [\"src_train\", \"train\", \"dev\", \"final\"]\n",
    "t_value_list = [5,10,20, 30, 40, 50, 100]\n",
    "addt_value_list=[0]\n",
    "\n",
    "noise_num = 300\n",
    "noise_num_sample = noise_num\n",
    "parallel_batch = noise_num_sample\n",
    "noise_batch_num =   ((noise_num-1)//  parallel_batch) + 1\n",
    "## this is 1 here\n",
    "\n",
    "batch_size=200\n",
    "repeated_times = 1\n",
    "guidance_scale_list = [1.0]\n",
    "\n",
    "input_noise_list = [np.random.normal(size=8).tolist() for _ in range(noise_num)]\n",
    "challenge_name = 'challenge_with_id.csv'\n",
    "challenge_label_name = 'challenge_label.csv'\n",
    "USE_BEST_CHECKPOINT=True\n",
    "# given_random = np.random.normal(size=8).tolist()\n",
    "# input_noise_list = [given_random for _ in range(noise_num)]\n",
    "\n",
    "# sample_num=6000\n",
    "test_set_ratio = float(1/3)\n",
    "test_model_num = 10\n",
    "\n",
    "######################################################\n",
    "## new global var setting\n",
    "## ----------------------------------------------------------------------------- \n",
    "## select several models as known, the rest totally unknown to simulate dev set\n",
    "import random\n",
    "\n",
    "\n",
    "all_indices = list(range(30))  # 0-29\n",
    "train_indices = random.sample(all_indices, int(30 - test_model_num))\n",
    "\n",
    "### data processing (collection)\n",
    "BASE_PATH = \"./tabddpm_black_box/train/tabddpm_\"\n",
    "# train\n",
    "df_train_merge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"train_with_id.csv\")) for t in train_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_train_challenge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"challenge_with_id.csv\")) for t in all_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_train_merge = df_train_merge[~df_train_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_train_challenge.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "\n",
    "# test\n",
    "test_indices = list(set(all_indices) - set(train_indices))\n",
    "df_test_merge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"train_with_id.csv\")) for t in test_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_test_challenge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"challenge_with_id.csv\")) for t in all_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_test_merge = df_test_merge[~df_test_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_test_challenge.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "### data processing (collection) done\n",
    "\n",
    "print(\"the number of merge train data\", len(df_train_merge))\n",
    "DATA_PER_MODEL = 3000\n",
    "total_data_num = DATA_PER_MODEL * 2 * int(30 - test_model_num)\n",
    "TEST_DATA_MODEL = 1000\n",
    "\n",
    "X_TRAIN = np.zeros([total_data_num, noise_num_sample*len(t_value_list)*len(addt_value_list)])\n",
    "X_LABEL = np.zeros([total_data_num])  \n",
    "# X_TEST = np.zeros([TEST_DATA_MODEL * 2  * test_model_num, noise_num_sample*len(t_value_list)])\n",
    "X_TEST = np.zeros([TEST_DATA_MODEL * 2  * test_model_num, noise_num_sample*len(t_value_list)*len(addt_value_list)])\n",
    "X_LABEL2 = np.zeros([TEST_DATA_MODEL * 2 * test_model_num]) \n",
    "\n",
    "## ----------------------------------------------------------------------------- \n",
    "\n",
    "guidance_scale_list = [1.0]\n",
    "res_list = np.zeros([len(t_value_list), len(guidance_scale_list)])\n",
    "\n",
    "# name_path = 'tabddpm_black_box_mia_res/guidance_fixed_noise_01'\n",
    "name_path = 'tabddpm_black_box_mia_res/guidance_fixed_noise_02'\n",
    "\n",
    "# regression_model = MLP(input_dim=noise_num_sample*len(t_value_list), hidden_dim=100).cuda()\n",
    "NUM_Epochs = 750\n",
    "regression_model = MLP(input_dim=noise_num_sample*len(t_value_list)*len(addt_value_list), hidden_dim=200).cuda()\n",
    "# if USE_BEST_CHECKPOINT:\n",
    "#     regression_model.load_state_dict(torch.load('best_model.pt'))\n",
    "selected_res_list = np.zeros([len(t_value_list), len(guidance_scale_list)])\n",
    "avg_res_list = np.zeros([len(t_value_list), len(guidance_scale_list)])\n",
    "med_res_list = np.zeros([len(t_value_list), len(guidance_scale_list)])\n",
    "plot_res = []\n",
    "# for guidance_id, guidance_scale in enumerate(guidance_scale_list):\n",
    "    # regression_model = MLP(input_dim=noise_num_sample, hidden_dim=100).cuda()\n",
    "sum_quantile = 0\n",
    "avg_selection = 0.0\n",
    "avg_res = 0.0\n",
    "med_res=0.0\n",
    "t_value = t_value_list[0]\n",
    "\n",
    "for noise_batch_id in tqdm(range(noise_batch_num)):\n",
    "    input_noise = input_noise_list[noise_batch_id * parallel_batch: (noise_batch_id+1) * parallel_batch]\n",
    "\n",
    "    final_tpr_at_fpr, final_tpr_at_fpr2 = main_function_process()\n",
    "    plot_res.append(final_tpr_at_fpr)\n",
    "    print(final_tpr_at_fpr, final_tpr_at_fpr2)\n",
    "\n",
    "print(plot_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lGhGsrlPV2Ty"
   },
   "source": [
    "## Scoring\n",
    "\n",
    "Let's see how the attack does on `train`, for which we have the ground truth.\n",
    "When preparing a submission, you can use part of `train` to develop an attack and a held-out part to evaluate your attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-UN3zfuPV2Ty"
   },
   "outputs": [],
   "source": [
    "tpr_at_fpr_list = []\n",
    "# for base_dir in [TABDDPM_DATA_DIR, TABSYN_DATA_DIR]:\n",
    "for base_dir in [TABDDPM_DATA_DIR]:\n",
    "    predictions = []\n",
    "    solutions  = []\n",
    "    root = os.path.join(base_dir, \"train\")\n",
    "    for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):\n",
    "        path = os.path.join(root, model_folder)\n",
    "        predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "        solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    solutions = np.concatenate(solutions)\n",
    "    \n",
    "    tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "    tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "    \n",
    "    print(f\"{base_dir.split('_')[0]} Train Attack TPR at FPR==10%: {tpr_at_fpr}\")\n",
    "\n",
    "final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_at_fpr_list = []\n",
    "index = 0\n",
    "for base_dir in [TABDDPM_DATA_DIR]:\n",
    "    predictions = []\n",
    "    solutions  = []\n",
    "    root = os.path.join(base_dir, \"train\")\n",
    "    model_folders = [item for item in os.listdir(root) if os.path.isdir(os.path.join(root, item))]\n",
    "    for model_folder in sorted(model_folders, key=lambda d: int(d.split('_')[1])):\n",
    "        if index not in test_indices:\n",
    "            index+=1\n",
    "            continue\n",
    "        path = os.path.join(root, model_folder)\n",
    "        predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "        solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "        index+=1\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    solutions = np.concatenate(solutions)\n",
    "    \n",
    "    tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "    tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "    \n",
    "    print(f\"{base_dir.split('_')[0]} Train Attack TPR at FPR==10%: {tpr_at_fpr}\")\n",
    "\n",
    "final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_at_fpr_list = []\n",
    "index = 0\n",
    "for base_dir in [TABDDPM_DATA_DIR]:\n",
    "    predictions = []\n",
    "    solutions  = []\n",
    "    root = os.path.join(base_dir, \"train\")\n",
    "    model_folders = [item for item in os.listdir(root) if os.path.isdir(os.path.join(root, item))]\n",
    "    for model_folder in sorted(model_folders, key=lambda d: int(d.split('_')[1])):\n",
    "        if index not in train_indices:\n",
    "            index+=1\n",
    "            continue\n",
    "        path = os.path.join(root, model_folder)\n",
    "        predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "        solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "        index+=1\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    solutions = np.concatenate(solutions)\n",
    "    \n",
    "    tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "    tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "    \n",
    "    print(f\"{base_dir.split('_')[0]} Train Attack TPR at FPR==10%: {tpr_at_fpr}\")\n",
    "\n",
    "final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = roc_curve(solutions, predictions)\n",
    "roc_auc = auc(fpr, tpr)  # 计算 AUC\n",
    "\n",
    "# 绘制曲线\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance line')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title(f'ROC Curve for Train Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9LZ-EhfV2Ty"
   },
   "source": [
    "## Packaging the submission\n",
    "\n",
    "Now we can store the predictions into a zip file, which you can submit to CodaBench. Importantly, we create a single zip file for dev and final. The structure of the submission is as follows:\n",
    "\n",
    "```\n",
    "└── root_folder\n",
    "    ├── tabsyn_white_box\n",
    "    │   ├── dev\n",
    "    │   │   └── tabsyn_#\n",
    "    │   │       └── prediction.csv\n",
    "    │   └── final\n",
    "    │       └── tabsyn_#\n",
    "    │           └── prediction.csv\n",
    "    └── tabddpm_white_box\n",
    "        ├── dev \n",
    "        │   └── tabddpm_#\n",
    "        │       └── prediction.csv\n",
    "        └── final \n",
    "            └── tabddpm_# \n",
    "                └── prediction.csv\n",
    "```\n",
    "**Note:** The `root_folder` can have any name but it is important all of the subdirectories follow the above structure and naming conventions. \n",
    "\n",
    "If a participant is looking to submit an attack for only one of TabSyn and TabDDPM, they can simply omit the other directory (ie `tabddpm_white_box` or `tabsyn_white_box` from the root_folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ats5N4AoV2Tz"
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(f\"black_box_single_table_submission.zip\", 'w') as zipf:\n",
    "    for phase in [\"dev\", \"final\"]:\n",
    "        for base_dir in [TABDDPM_DATA_DIR]:\n",
    "            root = os.path.join(base_dir, phase)\n",
    "            model_folders = [item for item in os.listdir(root) if os.path.isdir(os.path.join(root, item))]\n",
    "            for model_folder in sorted(model_folders, key=lambda d: int(d.split('_')[1])):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                if not os.path.isdir(path): continue\n",
    "\n",
    "                file = os.path.join(path, \"prediction.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    # Use `arcname` to remove the base directory and phase directory from the zip path\n",
    "                    arcname = os.path.relpath(file, os.path.dirname(base_dir))\n",
    "                    zipf.write(file, arcname=arcname)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`prediction.csv` not found in {path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated white_box_single_table_submission.zip can be directly submitted to the dev phase in the CodaBench UI. Although this submission contains your predictions for both the dev and final set, you will only receive feedback on your predictions for the dev phase. The predictions for the final phase will be evaluated once the competiton ends using the most recent submission to the dev phase."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "midst-models-9WC0e6Lb-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
