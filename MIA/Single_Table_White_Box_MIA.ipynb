{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ5My1aIV2Tv"
   },
   "source": [
    "# Membership Inference over Diffusion-models-based Synthetic Tabular Data (MIDST) Challenge @ SaTML 2025.\n",
    "\n",
    "This notebook will walk you through the process of how we achieve a good score in **White Box Single Table Competition**.\n",
    "\n",
    "We only work on **TabDDPM** in this task.\n",
    "\n",
    "This competition focuses on White Box MIA on tabular diffusion models trained on a single table transaction dataset. The schema of the transaction dataset is as follows:\n",
    "| trans_id | account_id | trans_date | trans_type | operation | amount  | balance  | k_symbol | bank | account |\n",
    "|----------|------------|------------|------------|-----------|---------|----------|----------|------|---------|\n",
    "| integer  | integer    | integer    | integer    | integer   | float   | float    | integer  | integer | integer |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Package Imports and Evironment Setup**\n",
    "\n",
    "Ensure that you have installed the proper dependenices to run the notebook. The environment installation instructions are available [here](https://github.com/VectorInstitute/MIDSTModels/tree/main/starter_kits). Now that we have verfied we have the proper packages installed, lets import them and define global variables:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Model preparation\n",
    "\n",
    "In white box single table competition, we directly use the given models for our membership inferences.\n",
    "\n",
    "**You do not need to do anything in this section for white box!!!!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Loss function\n",
    "\n",
    "In this part, we define the loss function used in our pipeline. The loss function should maximize the loss difference between training and hold-out samples. As the model may remember training data better, so they should have lower loss between predicted loss and real loss for train data. In this process, we fix the noises to control variables.\n",
    "\n",
    "Note: We will only list the loss function used finally in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "from functools import partial\n",
    "from typing import Callable, Any\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import multivariate_normal\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from data import get_challenge_points, get_training_points\n",
    "from metrics import get_tpr_at_fpr, get_tpr_at_fpr_cuda\n",
    "\n",
    "TABDDPM_DATA_DIR = \"tabddpm_white_box\"\n",
    "TABSYN_DATA_DIR = \"tabsyn_white_box\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define the loss function here\n",
    "# global variables are assigned in the main process\n",
    "def mixed_loss(diffusion, x, out_dict, noise=None, t=None, pt=None, return_random=False, no_mean=False):\n",
    "    global t_value\n",
    "    global input_noise\n",
    "    global parallel_batch\n",
    "\n",
    "    x_num = x[:, : diffusion.num_numerical_features]\n",
    "    x_cat = x[:, diffusion.num_numerical_features :]\n",
    "    if x_num.shape[1] > 0:\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_num)\n",
    "\n",
    "    noise = input_noise\n",
    "    noise_tensor = torch.tensor(noise, device='cuda', dtype=torch.float)\n",
    "    batch_noise = noise_tensor.repeat(x_num.shape[0], 1) \n",
    "\n",
    "    # there is actually no categorical classes, as we have examined the DM, so we just ignore x_cat here and later\n",
    "    x_num = x_num.repeat_interleave(parallel_batch, dim=0)\n",
    "    x_cat = x_cat.repeat_interleave(parallel_batch, dim=0)\n",
    "\n",
    "    b = x_num.shape[0]\n",
    "\n",
    "    log_x_cat_t = x_cat\n",
    "\n",
    "    device = x.device\n",
    "    if t is None:\n",
    "        t, pt = diffusion.sample_time(b, device, \"uniform\")\n",
    "    \n",
    "    if return_random:\n",
    "        return noise, t, pt \n",
    "\n",
    "    global addt_value\n",
    "    additional_t = t*0 + addt_value\n",
    "\n",
    "    # forward x_num_t with (t+additional_t) timestamps\n",
    "    x_num_t = diffusion.gaussian_q_sample(x_num, t+additional_t, noise=batch_noise)\n",
    "    \n",
    "    if not return_random:\n",
    "        current_t = t\n",
    "        # predict noises with t timestamps\n",
    "        predicted_noise = diffusion._denoise_fn(x_num_t, current_t, **out_dict)\n",
    "        current_loss = diffusion._gaussian_loss(predicted_noise, batch_noise, batch_noise, current_t, batch_noise)\n",
    "        transformed_current_loss = current_loss.reshape(-1, parallel_batch)\n",
    "\n",
    "    return transformed_current_loss*0, transformed_current_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Processing and Membership Inference\n",
    "\n",
    "Since raw loss values vary due to noise and different timesteps t_value and add, a simple threshold-based approach is insufficient for robust inference. To address this challenge, we propose a machine-learning-driven approach. Specifically, we introduce a three-layer Multi-Layer Perceptron (MLP) to model the relationship between loss values and membership status, improving attack accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Data Processing\n",
    "\n",
    "This part include all the source code used during preprocessing and dataset loading. We use these to preprocess the data to fit into diffusion models. Most of these functions are from TabDDPM with minor modifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "import os\n",
    "from typing import Any, Dict, List, Literal, Optional, Tuple, Union, cast\n",
    "from pathlib import Path\n",
    "from dataclasses import astuple, dataclass, replace\n",
    "from midst_models.single_table_TabDDPM.lib import (\n",
    "    Transformations,\n",
    "    prepare_fast_dataloader,\n",
    "    round_columns,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.lib import Dataset, TaskType, transform_dataset\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_clustering_force_load,\n",
    "    clava_load_pretrained_customized,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_utils import load_multi_table_customized\n",
    "\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import CustomUnpickler\n",
    "\n",
    "\n",
    "CAT_MISSING_VALUE = \"__nan__\"\n",
    "CAT_RARE_VALUE = \"__rare__\"\n",
    "Normalization = Literal[\"standard\", \"quantile\", \"minmax\"]\n",
    "NumNanPolicy = Literal[\"drop-rows\", \"mean\"]\n",
    "CatNanPolicy = Literal[\"most_frequent\"]\n",
    "CatEncoding = Literal[\"one-hot\", \"counter\"]\n",
    "YPolicy = Literal[\"default\"]\n",
    "ArrayDict = Dict[str, np.ndarray]\n",
    "\n",
    "def raise_unknown(unknown_what: str, unknown_value: Any):\n",
    "    raise ValueError(f\"Unknown {unknown_what}: {unknown_value}\")\n",
    "\n",
    "\n",
    "def get_table_info(df, domain_dict, y_col):\n",
    "    cat_cols = []\n",
    "    num_cols = []\n",
    "    for col in df.columns:\n",
    "        if col in domain_dict and col != y_col:\n",
    "            if domain_dict[col][\"type\"] == \"discrete\":\n",
    "                cat_cols.append(col)\n",
    "            else:\n",
    "                num_cols.append(col)\n",
    "\n",
    "    df_info = {}\n",
    "    df_info[\"cat_cols\"] = cat_cols\n",
    "    df_info[\"num_cols\"] = num_cols\n",
    "    df_info[\"y_col\"] = y_col\n",
    "    df_info[\"n_classes\"] = 0\n",
    "    df_info[\"task_type\"] = \"multiclass\"\n",
    "\n",
    "    return df_info\n",
    "\n",
    "def get_T_dict():\n",
    "    return {\n",
    "        \"seed\": 0,\n",
    "        \"normalization\": \"quantile\",\n",
    "        \"num_nan_policy\": None,\n",
    "        \"cat_nan_policy\": None,\n",
    "        \"cat_min_frequency\": None,\n",
    "        \"cat_encoding\": None,\n",
    "        \"y_policy\": \"default\",\n",
    "    }\n",
    "\n",
    "def get_model_params(rtdl_params=None):\n",
    "    return {\n",
    "        \"num_classes\": 0,\n",
    "        \"is_y_cond\": \"none\",\n",
    "        \"rtdl_params\": {\"d_layers\": [512, 1024, 1024, 1024, 1024, 512], \"dropout\": 0.0}\n",
    "        if rtdl_params is None\n",
    "        else rtdl_params,\n",
    "    }\n",
    "\n",
    "def build_target(\n",
    "    y: ArrayDict, policy: Optional[YPolicy], task_type: TaskType\n",
    ") -> Tuple[ArrayDict, Dict[str, Any]]:\n",
    "    info: Dict[str, Any] = {\"policy\": policy}\n",
    "    if policy is None:\n",
    "        pass\n",
    "    elif policy == \"default\":\n",
    "        if task_type == TaskType.REGRESSION:\n",
    "            mean, std = float(y[\"train\"].mean()), float(y[\"train\"].std())\n",
    "            y = {k: (v - mean) / std for k, v in y.items()}\n",
    "            info[\"mean\"] = mean\n",
    "            info[\"std\"] = std\n",
    "    else:\n",
    "        raise_unknown(\"policy\", policy)\n",
    "    return y, info\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Transformations:\n",
    "    seed: int = 0\n",
    "    normalization: Optional[Normalization] = None\n",
    "    num_nan_policy: Optional[NumNanPolicy] = None\n",
    "    cat_nan_policy: Optional[CatNanPolicy] = None\n",
    "    cat_min_frequency: Optional[float] = None\n",
    "    cat_encoding: Optional[CatEncoding] = None\n",
    "    y_policy: Optional[YPolicy] = \"default\"\n",
    "\n",
    "\n",
    "def transform_dataset(\n",
    "    dataset: Dataset,\n",
    "    transformations: Transformations,\n",
    "    cache_dir: Optional[Path],\n",
    "    transform_cols_num: int = 0,\n",
    "    normalizer=None,\n",
    "    cat_transform=None, \n",
    "    num_transform=None\n",
    ") -> Dataset:\n",
    "    # WARNING: the order of transformations matters. Moreover, the current\n",
    "    # implementation is not ideal in that sense.\n",
    "    if cache_dir is not None:\n",
    "        transformations_md5 = hashlib.md5(\n",
    "            str(transformations).encode(\"utf-8\")\n",
    "        ).hexdigest()\n",
    "        transformations_str = \"__\".join(map(str, astuple(transformations)))\n",
    "        cache_path = (\n",
    "            cache_dir / f\"cache__{transformations_str}__{transformations_md5}.pickle\"\n",
    "        )\n",
    "        if cache_path.exists():\n",
    "            cache_transformations, value = util.load_pickle(cache_path)\n",
    "            if transformations == cache_transformations:\n",
    "                return value\n",
    "            else:\n",
    "                raise RuntimeError(f\"Hash collision for {cache_path}\")\n",
    "    else:\n",
    "        cache_path = None\n",
    "\n",
    "\n",
    "    cat_transform = None\n",
    "    X_num = dataset.X_num\n",
    "    X_num = {k: num_transform.transform(v) for k, v in X_num.items()}\n",
    "\n",
    "    if dataset.X_cat is None:\n",
    "        assert transformations.cat_nan_policy is None\n",
    "        assert transformations.cat_min_frequency is None\n",
    "        # assert transformations.cat_encoding is None\n",
    "        X_cat = None\n",
    "    else:\n",
    "        X_cat = cat_process_nans(dataset.X_cat, transformations.cat_nan_policy)\n",
    "        if transformations.cat_min_frequency is not None:\n",
    "            X_cat = cat_drop_rare(X_cat, transformations.cat_min_frequency)\n",
    "        \n",
    "        if cat_transform is None:\n",
    "            raise ValueError(\"See why no cat_tramsform\")\n",
    "        else:\n",
    "            X_cat = {k: cat_transform.transform(v).astype(\"float32\") for k, v in X_cat.items()} \n",
    "            X_num = (\n",
    "                X_cat\n",
    "                if X_num is None\n",
    "                else {x: np.hstack([X_num[x], X_cat[x]]) for x in X_num}\n",
    "            )\n",
    "            X_cat = None\n",
    "\n",
    "    y, y_info = build_target(dataset.y, transformations.y_policy, dataset.task_type)\n",
    "\n",
    "    dataset = replace(dataset, X_num=X_num, X_cat=X_cat, y=y, y_info=y_info)\n",
    "    dataset.num_transform = num_transform\n",
    "    dataset.cat_transform = cat_transform\n",
    "\n",
    "    return dataset\n",
    "\n",
    "def make_dataset_from_df_with_loaded(df, T, is_y_cond, ratios=[0.7, 0.2, 0.1], df_info=None, std=0, label_encoders=None, num_transform=None):\n",
    "\n",
    "    cat_column_orders = []\n",
    "    num_column_orders = []\n",
    "    index_to_column = list(df.columns)\n",
    "    column_to_index = {col: i for i, col in enumerate(index_to_column)}\n",
    "\n",
    "    if df_info[\"n_classes\"] > 0:\n",
    "        X_cat = {} if df_info[\"cat_cols\"] is not None or is_y_cond == \"concat\" else None\n",
    "        X_num = {} if df_info[\"num_cols\"] is not None else None\n",
    "        y = {}\n",
    "\n",
    "        cat_cols_with_y = []\n",
    "        if df_info[\"cat_cols\"] is not None:\n",
    "            cat_cols_with_y += df_info[\"cat_cols\"]\n",
    "        if is_y_cond == \"concat\":\n",
    "            cat_cols_with_y = [df_info[\"y_col\"]] + cat_cols_with_y\n",
    "\n",
    "        if len(cat_cols_with_y) > 0:\n",
    "            X_cat[\"train\"] = df[cat_cols_with_y].to_numpy(dtype=np.str_)\n",
    "\n",
    "        y[\"train\"] = df[df_info[\"y_col\"]].values.astype(np.float32)\n",
    "\n",
    "        if df_info[\"num_cols\"] is not None:\n",
    "            X_num[\"train\"] = df[df_info[\"num_cols\"]].values.astype(np.float32)\n",
    "\n",
    "        cat_column_orders = [column_to_index[col] for col in cat_cols_with_y]\n",
    "        num_column_orders = [column_to_index[col] for col in df_info[\"num_cols\"]]\n",
    "\n",
    "    else:\n",
    "        X_cat = {} if df_info[\"cat_cols\"] is not None else None\n",
    "        X_num = {} if df_info[\"num_cols\"] is not None or is_y_cond == \"concat\" else None\n",
    "        y = {}\n",
    "\n",
    "        num_cols_with_y = []\n",
    "        if df_info[\"num_cols\"] is not None:\n",
    "            num_cols_with_y += df_info[\"num_cols\"]\n",
    "        if is_y_cond == \"concat\":\n",
    "            num_cols_with_y = [df_info[\"y_col\"]] + num_cols_with_y\n",
    "\n",
    "        if len(num_cols_with_y) > 0:\n",
    "            X_num[\"train\"] = df[num_cols_with_y].values.astype(np.float32)\n",
    "\n",
    "        y[\"train\"] = df[df_info[\"y_col\"]].values.astype(np.float32)\n",
    "\n",
    "        if df_info[\"cat_cols\"] is not None:\n",
    "            X_cat[\"train\"] = df[df_info[\"cat_cols\"]].to_numpy(dtype=np.str_)\n",
    "\n",
    "        cat_column_orders = [column_to_index[col] for col in df_info[\"cat_cols\"]]\n",
    "        num_column_orders = [column_to_index[col] for col in num_cols_with_y]\n",
    "\n",
    "    column_orders = num_column_orders + cat_column_orders\n",
    "    column_orders = [index_to_column[index] for index in column_orders]\n",
    "\n",
    "    if X_cat is not None and len(df_info[\"cat_cols\"]) > 0:\n",
    "        X_cat_all = X_cat[\"train\"]\n",
    "        X_cat_converted = []\n",
    "        for col_index in range(X_cat_all.shape[1]):\n",
    "            if label_encoders is None:\n",
    "                raise ValueError('Should be loaded: label_encoder')\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            X_cat_converted.append(\n",
    "                label_encoders[col_index].transform(X_cat_all[:, col_index]).astype(float)\n",
    "            )\n",
    "            \n",
    "            if std > 0:\n",
    "                # add noise\n",
    "                X_cat_converted[-1] += np.random.normal(\n",
    "                    0, std, X_cat_converted[-1].shape\n",
    "                )\n",
    "\n",
    "        X_cat_converted = np.vstack(X_cat_converted).T\n",
    "\n",
    "        train_num = X_cat[\"train\"].shape[0]\n",
    "\n",
    "        X_cat[\"train\"] = X_cat_converted[:train_num, :]\n",
    "\n",
    "        if len(X_num) > 0:\n",
    "            X_num[\"train\"] = np.concatenate((X_num[\"train\"], X_cat[\"train\"]), axis=1)\n",
    "        else:\n",
    "            X_num = X_cat\n",
    "            X_cat = None\n",
    "\n",
    "    D = Dataset(\n",
    "        X_num,\n",
    "        None,\n",
    "        y,\n",
    "        y_info={},\n",
    "        task_type=TaskType(df_info[\"task_type\"]),\n",
    "        n_classes=df_info[\"n_classes\"],\n",
    "    )\n",
    "\n",
    "    return transform_dataset(D, T, None, num_transform=num_transform), label_encoders, column_orders\n",
    "\n",
    "\n",
    "def get_dataset(config_path =None, save_dir_tmp=None, train_name=\"train_with_id.csv\", phase=None):\n",
    "    configs, save_dir = load_configs(config_path)\n",
    "    tables, relation_order, dataset_meta = load_multi_table_customized(save_dir_tmp, meta_dir=\"../midst_models/single_table_TabDDPM/configs\", train_name=train_name)\n",
    "    tables, all_group_lengths_prob_dicts = clava_clustering_force_load(\n",
    "        tables, relation_order, save_dir, configs\n",
    "    )\n",
    "    global batch_size\n",
    "    train_loader_list = []\n",
    "    for parent, child in relation_order:\n",
    "        # print(f\"Getting {parent} -> {child} model from scratch\")\n",
    "        df_with_cluster = tables[child][\"df\"]\n",
    "\n",
    "        id_cols = [col for col in df_with_cluster.columns if \"_id\" in col]\n",
    "        df_without_id = df_with_cluster.drop(columns=id_cols)\n",
    "\n",
    "        child_df_with_cluster, child_domain_dict, parent_name, child_name =  df_without_id, tables[child][\"domain\"], parent, child\n",
    "        if parent_name is None:\n",
    "            y_col = \"placeholder\"\n",
    "            child_df_with_cluster[\"placeholder\"] = list(range(len(child_df_with_cluster)))\n",
    "        else:\n",
    "            y_col = f\"{parent_name}_{child_name}_cluster\"\n",
    "        child_info = get_table_info(child_df_with_cluster, child_domain_dict, y_col)\n",
    "        child_model_params = get_model_params(\n",
    "            {\n",
    "                \"d_layers\": configs[\"diffusion\"][\"d_layers\"],\n",
    "                \"dropout\": configs[\"diffusion\"][\"dropout\"],\n",
    "            }\n",
    "        )\n",
    "        child_T_dict = get_T_dict()\n",
    "        file_path = os.path.join(save_dir_tmp, f\"{parent}_{child}_ckpt.pkl\")\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            model = CustomUnpickler(f).load()\n",
    "\n",
    "        diffusion = model['diffusion'].cuda()\n",
    "        \n",
    "        # important, dev and final model is different from train one, so retrive transform from here\n",
    "        if phase ==\"train\":\n",
    "            num_transform = model['dataset'].num_transform\n",
    "        elif phase == 'dev' or phase == 'final':\n",
    "            num_transform = model[\"inverse_transform\"].__self__\n",
    "        else:\n",
    "            raise ValueError(\"Unknown Phase!!!\")\n",
    "        T = Transformations(**child_T_dict)\n",
    "\n",
    "        dataset, label_encoders, column_orders = make_dataset_from_df_with_loaded(\n",
    "            child_df_with_cluster,\n",
    "            T,\n",
    "            is_y_cond=child_model_params[\"is_y_cond\"],\n",
    "            ratios=[0.99, 0.005, 0.005],\n",
    "            df_info=child_info,\n",
    "            std=0,\n",
    "            label_encoders=model['label_encoders'],\n",
    "            num_transform=num_transform\n",
    "        )\n",
    "        dataset.X_num['test'] = dataset.X_num['train']\n",
    "\n",
    "        if dataset.X_cat is not None:\n",
    "            dataset.X_cat['test'] = dataset.X_cat['train']\n",
    "        dataset.y['test'] = dataset.y['train']\n",
    "        train_loader = prepare_fast_dataloader(\n",
    "            dataset, split=\"test\", batch_size=batch_size, y_type=\"long\"\n",
    "        )\n",
    "        train_loader_list.append([train_loader, dataset.X_num['test'].shape[0], dataset])\n",
    "    return train_loader_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Get score\n",
    "\n",
    "This part we are trying to get a score based on any input models. The basic logit is to use the loss function before, to get the L2 distances of added noises and predicted noises. We need to use functions in data processing to obtain scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from midst_models.single_table_TabDDPM.complex_pipeline import clava_clustering\n",
    "import json\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import load_multi_table\n",
    "from midst_models.single_table_TabDDPM.complex_pipeline import (\n",
    "    clava_clustering,\n",
    "    clava_training,\n",
    "    clava_load_pretrained,\n",
    "    clava_synthesizing,\n",
    "    load_configs,\n",
    ")\n",
    "from midst_models.single_table_TabDDPM.pipeline_modules import child_training\n",
    "\n",
    "import os\n",
    "\n",
    "import sys\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# This function is used to get loss values from input data with given diffusion models\n",
    "def get_score(save_dir, config_path=None, type='tabddpm', phase=None):\n",
    "    \n",
    "    global challenge_name\n",
    "    if type == 'tabddpm':\n",
    "        relation_order=[(\"None\", \"trans\")]\n",
    "    elif type == 'tabsyn':\n",
    "        raise ValueError(\"Haven't done it yet!\")\n",
    "    \n",
    "    # load data from the data path\n",
    "    train_loader_list = get_dataset(config_path, save_dir, train_name=challenge_name, phase=phase)\n",
    "    \n",
    "    # for tabddpm, relation order only contains like None_trans\n",
    "    loader_count = 0\n",
    "    global noise_batch_id\n",
    "    global parallel_batch\n",
    "\n",
    "    for parent, child in relation_order:\n",
    "        assert os.path.exists(\n",
    "            os.path.join(save_dir, f\"{parent}_{child}_ckpt.pkl\")\n",
    "        )\n",
    "        train_loader, iter_max, challenge_dataset = train_loader_list[loader_count]\n",
    "\n",
    "        filepath = os.path.join(save_dir, f\"{parent}_{child}_ckpt.pkl\")        \n",
    "        \n",
    "        # get the diffusion model\n",
    "        with open(filepath, \"rb\") as f:\n",
    "            model = CustomUnpickler(f).load()\n",
    "        diffusion = model['diffusion'].cuda()\n",
    "\n",
    "        device = 'cuda'\n",
    "        iter_id = 0\n",
    "        global batch_size\n",
    "\n",
    "        iter_max = iter_max//batch_size\n",
    "        # return_res = torch.zeros([batch_size*parallel_batch, 1])\n",
    "        return_res = torch.zeros([batch_size, parallel_batch])\n",
    "        assert iter_max == 1\n",
    "        iter_id = 0\n",
    "        while iter_id < iter_max:\n",
    "            \n",
    "            x, out_dict = next(train_loader)\n",
    "            out_dict = {\"y\": out_dict}\n",
    "            x = x.to(device)\n",
    "            for k in out_dict:\n",
    "                out_dict[k] = out_dict[k].long().to(device)\n",
    "\n",
    "            # This part we want to fix the random variables noise, t, pt. So they are dealing as gloabl variable\n",
    "            global noise\n",
    "            global t\n",
    "            global t_value\n",
    "            global pt\n",
    "\n",
    "            # This loss_dataset is an indicator to show statistic information about the training data (And we want to show it only once)\n",
    "            global loss_dataset\n",
    "            with torch.no_grad():\n",
    "         \n",
    "                # get loss here\n",
    "                noise, _, pt =  mixed_loss (diffusion, x, out_dict, return_random=True)\n",
    "                t = _ * 0 + t_value\n",
    "                _, loss = mixed_loss (diffusion, x, out_dict, t=t, noise=noise, pt=pt, no_mean=True)\n",
    "\n",
    "            return_res = loss\n",
    "            iter_id += 1\n",
    "    return return_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Model definition\n",
    "\n",
    "Here, we define the 3-layer MLP model, and the training function. During training, we also evaluate the model's performances on validation sets periodically (each 10 epochs) (defined as X_test and y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torch.nn.init as init\n",
    "\n",
    "# here we define a MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = torch.tanh(self.fc1(x))  \n",
    "        residual = torch.tanh(self.fc2(residual))  \n",
    "        output = torch.sigmoid(self.fc3(residual))\n",
    "        return output\n",
    "\n",
    "\n",
    "def custom_loss_fn(model, X, y, fpr_target=0.5):\n",
    "    confidences = model(X)\n",
    "    X = X.float()\n",
    "    y = y.float()\n",
    "    mse_loss = nn.BCELoss()(confidences, y.unsqueeze(1))\n",
    "    return mse_loss\n",
    "\n",
    "\n",
    "# train the model here\n",
    "def fitmodel(regression_model, X_train, X_label, X_test, X_label2, fpr_target=0.5, num_epochs=1000, learning_rate=1e-4):\n",
    "    global test_set_ratio\n",
    "    global USE_BEST_CHECKPOINT\n",
    "    \n",
    "    optimizer = optim.Adam(regression_model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # -------------------------new data-------------------\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).cuda()\n",
    "    y_train = torch.tensor(X_label, dtype=torch.float32).cuda()\n",
    "    indices = torch.randperm(X_train.size(0)).cuda()\n",
    "    X_train = X_train[indices]\n",
    "    y_train = y_train[indices]\n",
    "    \n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).cuda()\n",
    "    y_test = torch.tensor(X_label2, dtype=torch.float32).cuda()\n",
    "    #  --------------------------------------------------- \n",
    "\n",
    "    X_train.requires_grad = True\n",
    "    y_train.requires_grad = True\n",
    "    train_loss_res = []\n",
    "    test_loss_res = []\n",
    "    train_tpr_res = []\n",
    "    test_tpr_res = []\n",
    "    epoch_plot = []\n",
    "    regression_model.train()\n",
    "    best_tpr = 0.0\n",
    "    for epoch in range(num_epochs):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = custom_loss_fn(regression_model, X_train, y_train, fpr_target)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()    \n",
    "        with torch.no_grad():\n",
    "            if (epoch+1) % 10 == 0:\n",
    "                train_loss_res.append(loss.item())\n",
    "                epoch_plot.append(epoch)\n",
    "                tpr_at_fpr = get_tpr_at_fpr(y_train.detach().cpu().numpy(), regression_model(X_train).detach().cpu().numpy())\n",
    "                train_tpr_res.append(tpr_at_fpr)\n",
    "\n",
    "                # if there is validation set\n",
    "                if test_set_ratio > 0:\n",
    "                    test_loss = custom_loss_fn(regression_model, X_test, y_test, fpr_target)\n",
    "                    test_tpr_at_fpr = get_tpr_at_fpr(y_test.detach().cpu().numpy(), regression_model(X_test).detach().cpu().numpy())\n",
    "                    test_loss_res.append(test_loss.item())\n",
    "                    test_tpr_res.append(test_tpr_at_fpr)\n",
    "                    if test_tpr_at_fpr> best_tpr:\n",
    "                        best_tpr = test_tpr_at_fpr\n",
    "                        torch.save(regression_model.state_dict(), 'best_model.pt')\n",
    "  \n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()} Test Loss :{test_loss.item()} Train TPR: {tpr_at_fpr} Test TPR: {test_tpr_at_fpr}\")\n",
    "                else:\n",
    "                    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {loss.item()} Train TPR: {tpr_at_fpr}\")\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epoch_plot, train_loss_res, label='Train Loss', color='blue')\n",
    "    if test_set_ratio > 0:\n",
    "        plt.plot(epoch_plot, test_loss_res, label='Test Loss', color='red')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Train and Test Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(epoch_plot, train_tpr_res, label='Train TPR', color='green')\n",
    "    if test_set_ratio > 0:\n",
    "        plt.plot(epoch_plot, test_tpr_res, label='Test TPR', color='orange')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.title('Train and Test TPR')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    if USE_BEST_CHECKPOINT:\n",
    "        regression_model.load_state_dict(torch.load('best_model.pt'))\n",
    "    if test_set_ratio > 0:\n",
    "        test_loss = custom_loss_fn(regression_model, X_test, y_test, fpr_target)\n",
    "        test_tpr_at_fpr =  get_tpr_at_fpr(y_test.detach().cpu().numpy(), regression_model(X_test).detach().cpu().numpy())\n",
    "        print(f'final best loss is {test_loss} best tpr is {test_tpr_at_fpr}')\n",
    "    return regression_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Data preparation\n",
    "\n",
    "In this section, we use the functions before to form a complete pipeline. \n",
    "\n",
    "1. It starts from tabular data splits and preparation, including training and validation sets from 30 train phase models. \n",
    "2. Then in the src_train phase, it generates a group of scores for each data according to the defined hyperparameters, and train the MLP model. \n",
    "3. After training, the codes iterate each phase [train, dev, final] to predict the score using MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "import copy\n",
    "import torch\n",
    "\n",
    "# this function is the main pipeline entrance\n",
    "def main_function_process():\n",
    "    global regression_model\n",
    "    global noise_num_sample\n",
    "    global t_value\n",
    "    global addt_value\n",
    "    global sample_num\n",
    "    global challenge_label_name\n",
    "    noise_count = 0\n",
    "    train_noise_count = 0\n",
    "    train_count = 0\n",
    "    test_count = 0\n",
    "    for base_dir, model_type in zip([TABDDPM_DATA_DIR], ['tabddpm']):\n",
    "        if model_type == 'tabddpm':\n",
    "            config_path = \"../midst_models/single_table_TabDDPM/configs/trans_demo.json\"\n",
    "        \n",
    "        for phase in phases:\n",
    "            # src_train phase is for training the MLP\n",
    "            if phase == 'src_train':\n",
    "                root = os.path.join(base_dir, \"train\")\n",
    "            else:\n",
    "                root = os.path.join(base_dir, phase)\n",
    "\n",
    "            global X_TRAIN\n",
    "            global X_LABEL\n",
    "            global X_TEST\n",
    "            global X_LABEL2\n",
    "            global challenge_name\n",
    "            global batch_size\n",
    "            global train_indices\n",
    "\n",
    "            index = 0\n",
    "            for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):                \n",
    "                global loss_dataset\n",
    "                global t\n",
    "                global pt\n",
    "\n",
    "                # Reset global varibale for each model\n",
    "                loss_dataset = False\n",
    "                t=None\n",
    "                pt=None\n",
    "                path = os.path.join(root, model_folder)\n",
    "                global df_train_merge\n",
    "                global df_test_merge\n",
    "                global DATA_PER_MODEL\n",
    "                global TEST_DATA_MODEL\n",
    "                                \n",
    "                if phase == 'src_train':\n",
    "                    # new data collection for training and validation\n",
    "                    ###########################################################\n",
    "\n",
    "                    # to train models, collect data to \"data.csv\"\n",
    "                    if index in train_indices:\n",
    "                        df_train = pd.read_csv(os.path.join(path, \"train_with_id.csv\"))\n",
    "                        \n",
    "                        # get data not chosen before and not in training set\n",
    "                        df_exclusive = df_train_merge[~df_train_merge.set_index([\"trans_id\", \"balance\"]).index.isin(\n",
    "                            df_train.set_index([\"trans_id\", \"balance\"]).index\n",
    "                        )]\n",
    "\n",
    "                        data_exclusive = df_exclusive.sample(DATA_PER_MODEL)\n",
    "                        data_from_train = df_train.sample(DATA_PER_MODEL)\n",
    "\n",
    "                        # store df_data in data.csv\n",
    "                        df_data = pd.concat([data_exclusive, data_from_train], ignore_index=True)\n",
    "                        df_data.to_csv(os.path.join(path, \"data.csv\"), index=False)\n",
    "\n",
    "                        # remove chosen data from df_train_merge\n",
    "                        df_train_merge = df_train_merge[~df_train_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_data.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "\n",
    "                    else:\n",
    "                        df_test = pd.read_csv(os.path.join(path, \"train_with_id.csv\"))\n",
    "                        df_exclusive = df_test_merge[~df_test_merge.set_index([\"trans_id\", \"balance\"]).index.isin(\n",
    "                            df_test.set_index([\"trans_id\", \"balance\"]).index\n",
    "                        )]\n",
    "\n",
    "                        data_test_exclusive = df_exclusive.sample(TEST_DATA_MODEL)\n",
    "                        data_from_test = df_test.sample(TEST_DATA_MODEL)\n",
    "\n",
    "                        # for store df_data in data.csv\n",
    "                        df_test_data = pd.concat([data_test_exclusive, data_from_test], ignore_index=True)\n",
    "                        df_test_data.to_csv(os.path.join(path, \"data.csv\"), index=False)\n",
    "\n",
    "                        # remove chosen data from df_test_merge\n",
    "                        df_test_merge = df_test_merge[~df_test_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_test_data.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "\n",
    "    \n",
    "                    t_value_count = 0\n",
    "                    t_value_count = 0\n",
    "                    for t_value in t_value_list:\n",
    "                        for addt_value in addt_value_list:\n",
    "                            if index in train_indices:  \n",
    "                                # train sets\n",
    "\n",
    "                                # define challenge_name (global variable) to make the model access that file\n",
    "                                challenge_name = \"data.csv\"\n",
    "                                # get predictions for these number of data\n",
    "                                batch_size = DATA_PER_MODEL * 2\n",
    "                                predictions = get_score(path, config_path, model_type, phase=\"train\") \n",
    "\n",
    "                                # store these losses to the corresponding positions, each data has an array of losses\n",
    "                                X_TRAIN[\n",
    "                                    DATA_PER_MODEL * 2 * train_count : DATA_PER_MODEL * 2 * (train_count + 1),\n",
    "                                    t_value_count * noise_num_sample : (t_value_count + 1) * noise_num_sample\n",
    "                                ] = (\n",
    "                                    predictions.detach().squeeze().cpu().numpy()\n",
    "                                )\n",
    "\n",
    "                                # the label is 1 for membership data and 0 for hold-out data\n",
    "                                X_LABEL[DATA_PER_MODEL*2*train_count : DATA_PER_MODEL*2*(train_count+1)] = np.concatenate([np.zeros(DATA_PER_MODEL), np.ones(DATA_PER_MODEL)])\n",
    "                                t_value_count += 1\n",
    "                            \n",
    "                            else:\n",
    "                                # validation sets \n",
    "                                challenge_name = \"data.csv\"\n",
    "                                batch_size = TEST_DATA_MODEL * 2\n",
    "                                predictions = get_score(path, config_path, model_type, phase=\"train\") \n",
    "                                \n",
    "                                X_TEST[\n",
    "                                    TEST_DATA_MODEL * 2 * test_count : TEST_DATA_MODEL * 2 * (test_count + 1),\n",
    "                                    t_value_count * noise_num_sample : (t_value_count + 1) * noise_num_sample\n",
    "                                ] = (\n",
    "                                    predictions.detach().squeeze().cpu().numpy()\n",
    "                                )\n",
    "\n",
    "                                X_LABEL2[TEST_DATA_MODEL*2*test_count : TEST_DATA_MODEL*2*(test_count+1)] = np.concatenate([np.zeros(TEST_DATA_MODEL), np.ones(TEST_DATA_MODEL)])\n",
    "                                t_value_count += 1\n",
    "\n",
    "                    # update index to locate the correct places in X_TRAIN (X_LABEL) / X_TEST (X_LABEL2)                         \n",
    "                    if index in train_indices:\n",
    "                        train_count += 1\n",
    "                        print(\"train\", train_count, index)\n",
    "                    else:\n",
    "                        test_count+=1\n",
    "                        print(\"test\", test_count, index)\n",
    "                    index += 1\n",
    "                    ###########################################################\n",
    "\n",
    "                else:\n",
    "                    batch_size = 200\n",
    "                    challenge_name = \"challenge_with_id.csv\"\n",
    "                    t_value_count = 0\n",
    "                    current_input = []\n",
    "                    for t_value in t_value_list:\n",
    "                        for addt_value in addt_value_list:\n",
    "                            predictions = get_score(path, config_path, model_type, phase=phase) \n",
    "                            t_value_count += 1\n",
    "                            current_input = current_input + [predictions]\n",
    "                    predictions = torch.cat(current_input, dim=-1)\n",
    "\n",
    "                    predictions = regression_model(predictions).detach().cpu().numpy()\n",
    "                    # clip to [0, 1]\n",
    "                    min_output, max_output = np.min(predictions), np.max(predictions)\n",
    "                    predictions = (predictions - min_output) / (max_output - min_output)\n",
    "                    predictions = torch.tensor(predictions)\n",
    "\n",
    "                    assert torch.all((0 <= predictions) & (predictions <= 1))\n",
    "\n",
    "                    with open(os.path.join(path, \"prediction.csv\"), mode=\"w\", newline=\"\") as file:\n",
    "                        writer = csv.writer(file)\n",
    "            \n",
    "                        # Write each value in a separate row\n",
    "                        for value in list(predictions.numpy().squeeze()):\n",
    "                            writer.writerow([value])\n",
    "\n",
    "            if phase == 'src_train':\n",
    "                # train the model\n",
    "                global NUM_Epochs\n",
    "                fitmodel(regression_model, X_TRAIN, X_LABEL, X_TEST, X_LABEL2, num_epochs=NUM_Epochs)\n",
    "    \n",
    "    # evaluate MIA performances in 30 train models\n",
    "    tpr_at_fpr_list = []\n",
    "    tpr_at_fpr2_list = []\n",
    "\n",
    "    for base_dir in [TABDDPM_DATA_DIR]:\n",
    "        predictions = []\n",
    "        predictions2 = []\n",
    "        solutions  = []\n",
    "        root = os.path.join(base_dir, \"train\")\n",
    "        global global_noise_count\n",
    "        global saved_tpr\n",
    "   \n",
    "        for i, model_folder in enumerate(sorted(os.listdir(root), key=lambda d: int(d.split('_')[1]))):\n",
    "            path = os.path.join(root, model_folder)\n",
    "            predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "            solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "        predictions = np.concatenate(predictions)\n",
    "        solutions = np.concatenate(solutions)\n",
    "        \n",
    "        tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "        tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "\n",
    "    final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "    final_tpr_at_fpr2 = 0\n",
    "    return final_tpr_at_fpr, final_tpr_at_fpr2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Run the pipeline\n",
    "\n",
    "This is the part you could run the MIA to get predictions for each challenge files.\n",
    "\n",
    "Mention: **run all the parts ahead before running this.**\n",
    "\n",
    "You may need about 35GB GPU memory to run using the default hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "phases = [\"src_train\", \"train\", \"dev\", \"final\"]\n",
    "\n",
    "# hyperparameters\n",
    "t_value_list = [5, 10, 20, 30, 40, 50, 100]\n",
    "addt_value_list=[0] # here addt is always 0, which means we are using the basic t loss functions, please refer to our white paper for details\n",
    "noise_num = 300\n",
    "\n",
    "Hidden_DIM = 200 # the dimension of the hidden layer\n",
    "NUM_Epochs = 5000 # number of training epochs\n",
    "DATA_PER_MODEL = 3000 # the number of train data (including DATA_PER_MODEL members and DATA_PER_MODEL hold-out) used in each model in train_indices\n",
    "TEST_DATA_MODEL = 1000 # the number of validation data used in each model not in train_indices\n",
    "# less important hyperparameters\n",
    "USE_BEST_CHECKPOINT=True # use the best checkpoint of the model evaluated at validation MIA TPR @ 0.1FPR\n",
    "test_model_num = 10\n",
    "# hyperparameters done\n",
    "\n",
    "noise_num_sample = noise_num\n",
    "parallel_batch = noise_num_sample\n",
    "noise_batch_num = 1\n",
    "batch_size=200\n",
    "repeated_times = 1\n",
    "\n",
    "# generate noises randomly\n",
    "input_noise_list = [np.random.normal(size=8).tolist() for _ in range(noise_num)]\n",
    "challenge_name = 'challenge_with_id.csv'\n",
    "challenge_label_name = 'challenge_label.csv'\n",
    "test_set_ratio = float(test_model_num/30)\n",
    "\n",
    "######################################################\n",
    "## new global var setting\n",
    "## ----------------------------------------------------------------------------- \n",
    "## select several models for training, the rest as validation sets\n",
    "import random\n",
    "\n",
    "all_indices = list(range(30))  # 0-29\n",
    "train_indices = random.sample(all_indices, int(30 - test_model_num))\n",
    "\n",
    "### data processing (merge)\n",
    "BASE_PATH = \"./tabddpm_white_box/train/tabddpm_\"\n",
    "# train\n",
    "df_train_merge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"train_with_id.csv\")) for t in train_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_train_challenge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"challenge_with_id.csv\")) for t in all_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_train_merge = df_train_merge[~df_train_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_train_challenge.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "\n",
    "# test\n",
    "test_indices = list(set(all_indices) - set(train_indices))\n",
    "df_test_merge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"train_with_id.csv\")) for t in test_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_test_challenge = pd.concat(\n",
    "    [pd.read_csv(os.path.join(BASE_PATH + str(t+1), \"challenge_with_id.csv\")) for t in all_indices], ignore_index=True\n",
    ").drop_duplicates(subset=[\"trans_id\", \"balance\"])\n",
    "\n",
    "df_test_merge = df_test_merge[~df_test_merge.set_index([\"trans_id\", \"balance\"]).index.isin\n",
    "                            (df_test_challenge.set_index([\"trans_id\", \"balance\"]).index)]\n",
    "### data processing (merge) done\n",
    "\n",
    "print(\"the number of merge train data\", len(df_train_merge))\n",
    "total_data_num = DATA_PER_MODEL * 2 * int(30 - test_model_num)\n",
    "\n",
    "# create np arrays for train data (+label) and test data (+label)\n",
    "X_TRAIN = np.zeros([total_data_num, noise_num_sample*len(t_value_list)*len(addt_value_list)])\n",
    "X_LABEL = np.zeros([total_data_num])  \n",
    "\n",
    "X_TEST = np.zeros([TEST_DATA_MODEL * 2  * test_model_num, noise_num_sample*len(t_value_list)*len(addt_value_list)])\n",
    "X_LABEL2 = np.zeros([TEST_DATA_MODEL * 2 * test_model_num]) \n",
    "## ----------------------------------------------------------------------------- \n",
    "\n",
    "# define the MLP here\n",
    "regression_model = MLP(input_dim=noise_num_sample*len(t_value_list)*len(addt_value_list), hidden_dim=Hidden_DIM).cuda()\n",
    "t_value = t_value_list[0]\n",
    "plot_res = []\n",
    "# call main_function_process to run the whole pipeline\n",
    "for noise_batch_id in tqdm(range(noise_batch_num)):\n",
    "    input_noise = input_noise_list[noise_batch_id * parallel_batch: (noise_batch_id+1) * parallel_batch]\n",
    "    final_tpr_at_fpr, final_tpr_at_fpr2 = main_function_process()\n",
    "    plot_res.append(final_tpr_at_fpr)\n",
    "    print(final_tpr_at_fpr, final_tpr_at_fpr2)\n",
    "\n",
    "print(plot_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring for MIA in train phase\n",
    "\n",
    "Let's see how the attack does on `train`, for which we have the ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tabddpm Train Attack TPR at FPR==10%: 0.42833333333333334\n",
      "Final Train Attack TPR at FPR==10%: 0.42833333333333334\n"
     ]
    }
   ],
   "source": [
    "tpr_at_fpr_list = []\n",
    "\n",
    "for base_dir in [TABDDPM_DATA_DIR]:\n",
    "    predictions = []\n",
    "    solutions  = []\n",
    "    root = os.path.join(base_dir, \"train\")\n",
    "    for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):\n",
    "        path = os.path.join(root, model_folder)\n",
    "        predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "        solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    solutions = np.concatenate(solutions)\n",
    "    \n",
    "    tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "    tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "    \n",
    "    print(f\"{base_dir.split('_')[0]} Train Attack TPR at FPR==10%: {tpr_at_fpr}\")\n",
    "\n",
    "final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for result in test_indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_at_fpr_list = []\n",
    "index = 0\n",
    "for base_dir in [TABDDPM_DATA_DIR]:\n",
    "    predictions = []\n",
    "    solutions  = []\n",
    "    root = os.path.join(base_dir, \"train\")\n",
    "    model_folders = [item for item in os.listdir(root) if os.path.isdir(os.path.join(root, item))]\n",
    "    for model_folder in sorted(model_folders, key=lambda d: int(d.split('_')[1])):\n",
    "        if index not in test_indices:\n",
    "            index+=1\n",
    "            continue\n",
    "        path = os.path.join(root, model_folder)\n",
    "        predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "        solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "        index+=1\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    solutions = np.concatenate(solutions)\n",
    "    \n",
    "    tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "    tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "    \n",
    "    print(f\"{base_dir.split('_')[0]} Train Attack TPR at FPR==10%: {tpr_at_fpr}\")\n",
    "\n",
    "final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for result in train_indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr_at_fpr_list = []\n",
    "index = 0\n",
    "for base_dir in [TABDDPM_DATA_DIR]:\n",
    "    predictions = []\n",
    "    solutions  = []\n",
    "    root = os.path.join(base_dir, \"train\")\n",
    "    model_folders = [item for item in os.listdir(root) if os.path.isdir(os.path.join(root, item))]\n",
    "    for model_folder in sorted(model_folders, key=lambda d: int(d.split('_')[1])):\n",
    "        if index not in train_indices:\n",
    "            index+=1\n",
    "            continue\n",
    "        path = os.path.join(root, model_folder)\n",
    "        predictions.append(np.loadtxt(os.path.join(path, \"prediction.csv\")))\n",
    "        solutions.append(np.loadtxt(os.path.join(path, \"challenge_label.csv\"), skiprows=1))\n",
    "        index+=1\n",
    "    \n",
    "    predictions = np.concatenate(predictions)\n",
    "    solutions = np.concatenate(solutions)\n",
    "    \n",
    "    tpr_at_fpr = get_tpr_at_fpr(solutions, predictions)\n",
    "    tpr_at_fpr_list.append(tpr_at_fpr)\n",
    "    \n",
    "    print(f\"{base_dir.split('_')[0]} Train Attack TPR at FPR==10%: {tpr_at_fpr}\")\n",
    "\n",
    "final_tpr_at_fpr = max(tpr_at_fpr_list)\n",
    "print(f\"Final Train Attack TPR at FPR==10%: {final_tpr_at_fpr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = roc_curve(solutions, predictions)\n",
    "roc_auc = auc(fpr, tpr)  \n",
    "\n",
    "# ROC curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.4f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Chance line')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title(f'ROC Curve for Train Data')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9LZ-EhfV2Ty"
   },
   "source": [
    "## Packaging the submission\n",
    "\n",
    "Now we can store the predictions into a zip file, which you can submit to CodaBench. Importantly, we create a single zip file for dev and final. The structure of the submission is as follows:\n",
    "\n",
    "```\n",
    " root_folder\n",
    "     tabsyn_white_box\n",
    "        dev\n",
    "           tabsyn_#\n",
    "               prediction.csv\n",
    "        final\n",
    "            tabsyn_#\n",
    "                prediction.csv\n",
    "     tabddpm_white_box\n",
    "         dev \n",
    "            tabddpm_#\n",
    "                prediction.csv\n",
    "         final \n",
    "             tabddpm_# \n",
    "                 prediction.csv\n",
    "```\n",
    "**Note:** The `root_folder` can have any name but it is important all of the subdirectories follow the above structure and naming conventions. \n",
    "\n",
    "If a participant is looking to submit an attack for only one of TabSyn and TabDDPM, they can simply omit the other directory (ie `tabddpm_white_box` or `tabsyn_white_box` from the root_folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ats5N4AoV2Tz"
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(f\"white_box_single_table_submission.zip\", 'w') as zipf:\n",
    "    for phase in [\"dev\", \"final\"]:\n",
    "        for base_dir in [TABDDPM_DATA_DIR]:\n",
    "            root = os.path.join(base_dir, phase)\n",
    "            for model_folder in sorted(os.listdir(root), key=lambda d: int(d.split('_')[1])):\n",
    "                path = os.path.join(root, model_folder)\n",
    "                if not os.path.isdir(path): continue\n",
    "\n",
    "                file = os.path.join(path, \"prediction.csv\")\n",
    "                if os.path.exists(file):\n",
    "                    # Use `arcname` to remove the base directory and phase directory from the zip path\n",
    "                    arcname = os.path.relpath(file, os.path.dirname(base_dir))\n",
    "                    zipf.write(file, arcname=arcname)\n",
    "                else:\n",
    "                    raise FileNotFoundError(f\"`prediction.csv` not found in {path}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated white_box_single_table_submission.zip can be directly submitted to the dev phase in the CodaBench UI. Although this submission contains your predictions for both the dev and final set, you will only receive feedback on your predictions for the dev phase. The predictions for the final phase will be evaluated once the competiton ends using the most recent submission to the dev phase."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "midst_models",
   "language": "python",
   "name": "midst_models"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
